---
title: 概率分布中的距离度量
date: 2020-08-26 17:38:32 
tags: KL散度，JS散度，推土机距离
categories: 统计学习
mathjax: true
toc: true
---
在$k$近邻章节中，我总结了在样本空间中常用的两种距离度量:$L_p$距离以及马氏距离。但在一些机器学习任务中，我们期望能够得到一个分布与目标分布尽可能接近，这也就引出了一个新的问题:
> 如何度量两个概率分布之间的距离(差异程度)？

本文将介绍一些常见的概率分布之间的距离度量，按照以下结构组织:
- 信息量与熵
- KL散度(相对熵)
- 交叉熵
- JS散度
- 推土机理论与Wasserstein距离
<!--more-->

### 信息量与熵 
#### 信息量
任何事件都会承载着一定的信息量，包括已经发生的事件和未发生的事件，只是它们承载的信息量有所不同：
> 🌰：对于两个事件：昨天下雨与明天下雨。昨天下雨这个事件，因为已经发生，既定事实，那么它的信息量就为0；而对于明天会下雨这个事件，因为未有发生，那么该事件的信息量就大。

从上面栗子可以看出信息量是一个与事件发生概率相关的概念，而且可以得出，事件发生的概率越小，其信息量越大。这也很好理解，因为所谓的“搞个大新闻”,也就是发生了不太可能发生的事情，"大新闻"则意味着该事件蕴含着更多的信息。从这个角度出发， 定义信息量:
> **信息量:** 假设$X$是一个离散型随机变量，其取值集合为$\mathcal{X}$，其概率分布为:
> $$
    p(x) = P(X = x), x \in \mathcal{X}
> $$
> 则定义事件$X = x_0$的信息量为:
> $$
    I(x_0) = -\log(p(x_0))
> $$

#### 熵 
首先给出熵的定义：
> 在信息论与概率统计中，熵是表示**随机变量不确定性**的度量，设$X$是一个取有限个值的离散随机变量，其概率分布为:
> $$
    P(X = x_i) = p_i, \quad i =1,2,\dots,n
> $$
则随机变量$X$的熵定义为:
$$
    H(X) = - \sum_{i=1}^n p_i \log p_i 
$$

有了信息量的定义后，再看关于熵的定义式可以看出，熵不过是在概率分布$P$下对信息量求的期望，所以说熵实际上就是**事件信息量的期望**。
若$p_i = 0$,则定义$0\log 0 =0$,式中的对数一般取以2为底或者以$e$为底，这时熵的单位被称作比特或纳特，由定义可知，熵只依赖于$X$的分布，而与$X$的取值无关，因此也可将$X$的熵记作$H(p)$,熵越大，随机变量的不确定性越大。因为$p_i \leq 0$,由$H(p)$函数性质(凹函数，应用拉格朗日乘子法可求最大值)可知:
$$
    0 \leq H(p) \leq \log n
$$

### KL散度(相对熵)
KL散度又称相对熵，如果我们对同一个随机变量有两个独立的概率分布$P(x)$和$Q(x)$，我们可以使用KL散度来衡量这两个分布的差异,下面给出KL散度的定义式:
> 在机器学习中，$P$往往用来表示样本的真实分布，$Q$用来表示模型所预测的分布，那么$KL$散度就可以计算两个分布之间的差异，也就是Loss损失值:
> $$
    \begin{aligned}
         D_{KL}(P||Q) &= -\sum_{i=1}^n p(x_i) \log(\frac{q(x_i)}{p(x_i)}) \\
            &= -\sum_{i=1}^n p(x_i) \log(q(x_i)) - (-\sum_{i=1}^n p(x_i) \log p(x_i))
    \end{aligned}
> $$
> 由于$-\log x$为凸函数，由凸函数性质可知:
> $$
     -\sum_{i=1}^n p(x_i) \log(\frac{q(x_i)}{p(x_i)}) \geq  \log \sum_{i=1}^n q(x_i) = 0 
> $$
> 这说明KL散度具有非负的性质，这也说明$P(x)$与其他分布的交叉熵恒大于熵。

这里需要说明的是，虽然KL散度具有非负性，但其并不满足度量的另外两个性质:
- 对称性
- 三角不等式

### 交叉熵
KL散度由两部分组成，第一部分是交叉熵，第二部分则是$P(x)$的熵，$P(x)$因为是真实的分布，所以其熵为固定值。在机器学习任务中，我们需要评估样本固有label与我们算法的预测$predict$之间的差距，在优化过程中，我们只需关注交叉熵，所以在分类任务中，往往采用交叉熵作为loss函数。

### JS散度 
JS散度只是KL散度的变形，其解决了KL散度的非对称问题，其表达式如下:
$$
    JS(P||Q) = \frac{1}{2} KL(P(x)||\frac{P(x) + Q(x)}{2}) + \frac{1}{2}KL(Q(x)||\frac{P(x)+ Q(x)}{2})
$$
若$\log$以2为底，则JS散度的范围为$[0,1]$,若$\log$以$e$为底，则JS散度范围为$[0,\ln 2]$。当两个分布完全相同时取下界，当两个分布完全不同时取上界。
这也就带来了一个问题，若优化时两个分布完全没有交集时,JS散度就是一个常数($1$或$\ln 2$),此时会出现梯度消失问题，导致无法进行优化。

### Wasserstein距离
首先说明下KL散度与JS散度存在的问题:
> 若两个分布$P,Q$完全没有重叠，则会导致KL散度无意义($\log 0$),而JS散度是一个常数，也就是说出现了梯度消失问题。

而Wasserstein距离可以一定程度上解决该问题。
#### 思想
对于两个分布$P$和$Q$，考虑从将一个分布转换成另一个分布所需的代价入手来定义概率分布的差异。下面以一个栗子来说明该距离的思想:
> 🌰: 考虑两个分布$P$和$Q$:
> $$
    
> $$