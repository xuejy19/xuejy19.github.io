---
title: 贝叶斯分类补充
date: 2020-10-05 20:15:30
tags: 贝叶斯分类，逻辑回归
categories: 统计学习 
toc: true
mathjax: true 
---
在前面我已经介绍过有关贝叶斯决策的相关知识，在进行实际应用时如果假设在给定类别情况下，各特征是彼此独立的:
$$
    Ind(x_1,x_2, \dots, x_d |y)
$$
则这样导出的分类器称为朴素贝叶斯分类器，这部分我主要总结一些贝叶斯分类器中容易忽视的知识点,具体的推导部分前面已经总结，这里不再给出:
<!--more-->
> - 贝叶斯分类器是最优分类器，分类错误率最小。
> - 朴素贝叶斯模型是贝叶斯网络的一种特殊形式。
> - 常用的参数估计方法有极大似然估计和贝叶斯估计，使用贝叶斯估计要求拥有待估计参数的先验知识，有助于缓解参数估计的过拟合。
> - 朴素贝叶斯模型可以应用离散特征也可应用于连续特征，但这时需要给出概率密度$p(x|y)$的形式，常用的是高斯分布。

下面则讨论一下朴素贝叶斯算法与$logistic$回归的关系:
> - 朴素贝叶斯模型是生成式模型，显式地给出了$p(y)$和$p(x|y)$的形式，即给出了数据生成所需的参数，在进行决策时采用贝叶斯公式计算后验概率$p(y|x)$进行决策。
> - $logistic$回归是判别式模型，直接以$sigmoid$函数的形式给出$p(y|x)$，然后学习函数中相关参数进行决策。
> - 当采用朴素贝叶斯分类器进行分类，$p(x｜y)$的形式为高斯形式，且假设对于不同类别，高斯分布的方差满足:
$$
     \Sigma_1 = \Sigma_2 = diag(\sigma_1, \sigma_2, \dots, \sigma_d)
$$
则应用贝叶斯公式可以推出:
$$
    p(y=1|x,\pi,\mu,\Sigma) = \frac{1}{1 + \exp(-w^Tx - w_0)}
$$
在形式上与$logistic$回归等价。
> - 朴素贝叶斯算法是通过参数估计的方法来估计概率密度函数中的参数，而$logistic$回归则是通过迭代优化的方法来求解一个凸优化问题。 
> - 对于有限数据，如果特征条件独立性满足，则两种模型表现接近，若条件独立性并不满足，则$logistic$回归模型表现要优于朴素贝叶斯模型。 
> - 高斯朴素贝叶斯模型相较于$logistic$回归模型，一般需要的数据更少，但随着数据量的增多，$logistic$回归模型表现一般会超过朴素贝叶斯模型。

最后从线性分类器与非线性分类器的角度来进行讨论:

> - 朴素贝叶斯分类器是否是线性分类器取决于一些参数共享假设，如果假设不同类别特征共享一个$\Sigma$，则此时导出的决策面就是一个超平面，而如果不共享方差参数，则导出的分类面不再是平面，此时朴素贝叶斯分类器便不是线性分类器。
> - 对于$logistic$回归，若是二分类，则是线性分类器。若是分类类别大于2，则最终的分类器是分段线性的，即任意两个类别之间是线性超平面。
![朴素贝叶斯](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/NB2.png)