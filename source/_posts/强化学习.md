<!--
 * @Description: 
 * @Version: V1.0
 * @Author: xuejy19@mails.tsinghua.edu.cn
 * @Date: 2020-10-06 17:43:59
-->
---
title: 强化学习
date: 2020-10-06 17:43:59
tags: 强化学习 
categories: 统计学习 
toc: true
mathjax: true 
---
这部分将根据周志华老师的西瓜书对强化学习部分做一个简单介绍，该部分按照以下章节进行组织:
- 思想介绍——任务与奖赏 
- K-摇臂赌博机 
- 有模型学习 
- 免模型学习 
- 值函数近似 
- 模仿学习 
<!--more-->

### 思想介绍-任务与奖赏 
![强化学习](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/reinforce.png) 
考虑这样病人看病的场景，假设我们是看病的医生，在我们与病人的交互过程中，大概是这么一个交互顺序:
- 病人本身有一个状态，记做$S$，表示病人目前的身体状况。
- 在看病的过程中，我认为病人处于某个状态$S'$，然后对病人施加了一个动作A，比如给病人吃了一些药，或者打了针。
- 在病人接受了治疗之后，其本身状态发生了一个变化，该状态的转移我们用概率矩$P$来表示，表示病人状态发生了一个变化，比如痊愈了或者病情更加严重了。 
- 在病人的状态发生了变化之后，会给作为医生的我们一个反馈，记做$R$,如果病人痊愈了，那我们得到的反馈可能会是一面锦旗和很多钱，如果病情更加严重了，我们得到的反馈可能就是埋怨。 

上面我所描述的场景其实就构成了一轮强化学习的过程，强化学习任务常用马尔可夫决策过程来描述，这意味着环境下一时刻的状态仅仅与当前时刻状态有关而与之前状态无关:
$$
    P(s_{t+1}| s_t,\dots, s_1) = P(s_{t+1}|s_{t})
$$ 
综合起来，强化学习对应了四元组$E = <S,A,P,R>$,其中:
$$
    P: X \times A \times X \rightarrow \mathbb{R} 
$$
而:
$$
    R: X \times A \times X \rightarrow \mathbb{R} 
$$
指定了奖赏，在有的应用中，状态可能仅与状态转移有关，即$R: X \times X \rightarrow \mathbb{R}$, 下面给出一个种瓜的马尔可夫决策过程的栗子。 
![马尔可夫决策过程](https://github.com/xuejy19/xuejy19.github.io/blob/source/Img/Markov_decision.png)
在该过程中只有四个状态:健康、缺水、溢水和凋亡和两个动作:浇水和不浇水。在每一步转移后，若状态是保持瓜苗健康则获得奖励1，瓜苗缺水或者溢水奖赏为-1，这时通过浇水或者不浇水可以让瓜苗恢复健康状态，当瓜苗凋零时奖赏是最小值-100且无法恢复。图中箭头表示状态转移，$a、p、r$分别表示导致状态转移的动作、转移概率以及返回的奖赏。容易看出，最优策略是在“健康”和“缺少”状态下选择浇水，而在“溢水”状态下选择不浇水。

需要注意区分“机器”和“环境”的界限，在下棋中，棋盘是环境与对手；在机器人控制中，环境是机器人的躯体与物理世界。总之，在环境中状态的转移以及奖赏的返回是不受机器控制的，机器只能通过选择要执行的动作来影响环境，也只能通过观察转移后的状态和返回的奖赏来感知世界。

机器要做的是学习到一个策略$\pi$，根据这个策略，在状态$x$下就能得知想要执行的动作$a = \pi(x)$, 例如看到瓜苗状态为缺水时，能返回动作“浇水”。策略有两种表示:
- 一种是将策略表示为函数$\pi: X \rightarrow A$,确定性策略通常用这种表示。 
- 一种是概率表示$\pi: X \times A \rightarrow R$ ,随机性概率通常用这种表示，$\pi(x,a)$为状态$x$下选择动作$a$的概率，这里必须有$\sum_a \pi(x,a) = 1$

策略的优劣取决于长期执行这一策略后得到的累计奖赏，例如某个策略使得瓜苗枯死，那么它的累积奖赏会很小，另一个策略种出了好瓜，它的累积奖赏会很大。在强化学习中，学习的目的就是要找到能使长期累积奖赏最大化的策略。长期累积奖赏有多种计算方式，常用的有两种:
- T步累积奖赏$E[\frac{1}{T} \sum_{t=1}^T r_t]$
- $\gamma$折扣累积奖赏: $E[\sum_{t=0}^\infty] \gamma^t r_{t+1}$, 其中$\gamma$表示贴现系数。 

最后讨论下强化学习与监督学习的关系:
- 强化学习中的“状态”对应于监督学习中的“示例”。
- 强化学习中的“动作”对应于监督学习中的“标记” 
- 强化学习中的“策略”实际上就相当于监督学习中的“分类器”或“回归性” 

但是在强化学习中，并没有监督学习中的有标记样本，换言之，没有人直接告诉机器在什么状态下应该做什么动作，只有等到最终结果揭晓，才能通过“反思”之前的动作是否正确来进行学习，因此，强化学习在某种意义上可以看作具有“延迟标记信息”的监督学习问题。 