---
title: logistic回归与最大熵模型
date: 2020-08-23 15:16:31
tags: logistic回归, 最大熵模型, 优化学习
categories: 统计学习
toc: true
mathjax: true
---
在学习李航老师《统计学习》条件随机场章节时，对于学习算法感到有些陌生，后来发现在书中第六章“logistic回归与最大熵模型”有过一些介绍，因此本章节便总结一下相关知识，其中logistic回归模型做简要介绍，重点放在最大熵模型的学习算法上，本文按照如下结构组织：
- logistic回归
- 最大熵模型
- 模型学习的最优化算法
<!--more-->
### logistic回归
#### logistic分布
首先介绍下logistic分布：
> **logistic分布**:设$X$是连续变量，$X$服从logistic分布是指$X$具有下列分布函数和密度函数：
> $$
    \begin{aligned}
        F(x) &= P(X \leq x) = \frac{1}{1 + e^{-\frac{x-\mu}{\gamma}}} \\
        f(x) &= F'(x) = \frac{e^{-\frac{x-\mu}{\gamma}}}{\gamma(1+e^{-\frac{x-\mu}{\gamma}})^2}
    \end{aligned}
> $$
> 式中，$\mu$为位置参数，$\gamma>0$为形状参数 

对于logistic分布，其概率密度函数和分布函数如下图所示：
![logistic](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/logistic.jpeg)
分布函数的图形是一条$S$形曲线，该曲线以点$(\mu,\frac{1}{2})$为中心对称，即满足:
$$
    F(-x+\mu) - \frac{1}{2} = -F(x+\mu) +\frac{1}{2}
$$
曲线在中心增长速度较快，在两端增长速度慢，形状参数$\gamma$的值越小，曲线在中心附近增长得越快。
#### 二项logistic回归模型
二项logistic回归模型是一种二分类模型，由条件概率分布$P(Y|X)$表示，形式为参数化的logistic分布，随机变量$X$的取值为实数，随机变量$Y$取值为1或0，下面给出logistic回归模型定义：
> **logistic回归模型**：二项logistic回归模型是如下条件概率分布：
> $$
    \begin{aligned}
        P(Y=1|x) &= \frac{exp(w \cdot x + b)}{1 + exp(w \cdot x + b)} \\
        P(Y=0|x) &= \frac{1}{1 + exp(w \cdot x + b)} 
    \end{aligned}
> $$
> 其中，$x \in R^n$是输入，$Y \in \{0,1\}$是输出，$w \in R^n,b \in R$是参数。

若我们已经有了logistic模型，则判断样本点$x$属于哪一类只需要比较两个概率值的大小，下面给出对数几率的定义:
> **对数几率**：一个事件的几率是指该事件发生的概率与该事件不发生的概率的比值，如果一个事件发生的概率是$p$,那么该事件的几率是$\frac{p}{1-p}$,该事件的对数几率或logit函数是:
> $$
    logit(p) = log \frac{p}{1-p}
> $$

对于logistic回归而言，事件$(Y=1|X)$的对数几率为：
$$
    log \frac{P(Y=1|x)}{1 - P(Y=1|x)} = w \cdot x
$$
这说明，logistic回归模型可以看做是用线性回归模型去逼近真实标记的对数几率。因此logistic回归模型也被称为对数几率回归，可以看作是经过了如下的映射环节，将$R^n$空间中的$x$映射到了其标记的概率：
$$
    x \in R^n  \rightarrow w \cdot x \in R \rightarrow P(Y=1|x) \in [0,1]
$$

#### 模型参数估计
对于logistic回归模型，需要估计的参数是$w,b$,可以考虑将它们合并为一个$n+1$维向量:
$$
    w = [w;b]
$$
同时，给输入向量加一维，表示为:
$$
    x = [x;1]
$$
这样logistic回归模型就可以表示成更加紧凑的形式:
$$
    \begin{aligned}
        P(Y=1|x) &=  \frac{exp(w \cdot x) }{1+exp(w \cdot x)} \\
        P(Y=0|x) &=  \frac{1}{1 + exp(w \cdot x)}
    \end{aligned}
$$
logistic模型参数学习属于有监督的模型学习问题范畴，利用极大似然法就可以解决，设：
$$
    P(Y=1|x) = \pi(x), \quad P(Y=0|x) = 1- \pi(x)
$$
似然函数可以写成:
$$
    \prod_{i=1}^N [\pi(x_i)]^{y_i} [1 - \pi(x_i)]^{1-y_i}
$$
对数似然函数为:
$$
    \begin{aligned}
        L(w) &= \sum_{i=1}^N [y_i log \pi(x_i) + (1-y_i)log(1-\pi(x_i))] \\
            &= \sum_{i=1}^N [y_i log \frac{\pi(x_i)}{1 - \pi(x_i)} + log(1-\pi(x_i))] \\
            &= \sum_{i=1}^N [y_i(w \cdot x) - log(1 + exp(w \cdot x))]
    \end{aligned}
$$
对$L(w)$求极大值，得到$w$的估计值，常用梯度下降法及拟牛顿法求解(后面专开一章讲解)。
#### 多项logistic回归
上面介绍的是二项logistic模型，适用于二分类，可以将其推广为多项logistic分类模型，用于多类分类。假设离散型随机变量$Y$的取值集合是$\{1,2,\dots,K\}$,那么多项logistic回归的模型是:
$$
    \begin{aligned}
        P(Y=k|x) &= \frac{\exp(w_k \cdot x)}{1 + \sum_{k=1}^{K-1} \exp(w_k \cdot x)} \\
        P(Y=K|x) &= \frac{1}{1 + \sum_{k=1}^{K-1} \exp(w_k \cdot x)}
    \end{aligned}
$$
这里$x \in R^{n+1},w_k \in R^{n+1}$。二项logistic回归的参数估计法也可以推广到多项logistic回归

### 最大熵模型
最大熵模型，是由最大熵原理推导实现。本部分按照以下结构组织:
- 最大熵原理
- 最大熵模型推导
- 最大熵模型学习

#### 最大熵原理
最大熵原理是概率模型学习的一个准则，最大熵原理认为：
> 学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。

我们首先来复习下信息论中关于熵的定义：
> 在信息论与概率统计中，熵是表示**随机变量不确定性**的度量，设$X$是一个取有限个值的离散随机变量，其概率分布为:
> $$
    P(X = x_i) = p_i, \quad i =1,2,\dots,n
> $$
则随机变量$X$的熵定义为:
$$
    H(X) = - \sum_{i=1}^n p_i \log p_i 
$$

若$p_i = 0$,则定义$0\log 0 =0$,式中的对数一般取以2为底或者以$e$为底，这时熵的单位被称作比特或纳特，由定义可知，熵只依赖于$X$的分布，而与$X$的取值无关，因此也可将$X$的熵记作$H(p)$,熵越大，随机变量的不确定性越大。因为$p_i \leq 0$,由$H(p)$函数性质(凹函数，应用拉格朗日乘子法可求最大值)可知:
$$
    0 \leq H(p) \leq \log n
$$
当且仅当$X$是均匀分布时右侧等号成立,也就是说，当$X$服从均匀分布时，熵最大，该随机变量的不确定性最大。直观的，最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件。在没有更多信息情况下，那些不确定的部分都是“等可能的”,最大熵原理通过熵的最大化来表示等可能性。下面通过一个简单的例子来介绍最大熵原理：
> 🌰：假设随机变量$X$有五个取值$\{A,B,C,D,E \}$,要估计取各个值的概率$P(A),P(B),P(C),P(D),P(E)$
> - 不提供任何额外约束，则只有这些概率的固有约束
> $$
    P(A) + P(B) + P(C) + P(D) + P(E) = 1
> $$
> 因为没有任何额外的信息，则根据最大熵原理，只能够认为该概率分布为均匀分布：
> $$
    P(A) = P(B) = P(C) = P(D) = P(E) = \frac{1}{5}
> $$
> - 从一些先验知识中得到了一些对概率值的约束条件，如$P(A) + P(B) = \frac{3}{10}$,满足约束的概率分布仍旧有无穷多个，在缺少其他信息的情况下，可以认为$A$和$B$是等概率的，$C,D,E$等概率。

实际上，最大熵原理就是为我们提供了一种选择最优概率模型的准则(保守)

#### 最大熵模型的定义