---
title: 提升算法总结
date: 2020-09-01 09:25:02
tags: Bagging, AdaBoost, 提升树模型
categories: 统计学习
mathjax: true 
toc: true 
---
对于一个特定机器学习问题，我们可能会建立很多模型，这些单个模型可能表现都不是非常好，由此便会引出一个问题:
> **问题1: 能否通过一个算法将这些模型组合起来(Ensemble)，产生一个效果更好的组合模型？**

这个问题的答案是肯定的，历史上，Kearns 和 Valiant首先提出了“弱可学习”和“强可学习”的概念：
> **强可学习：** 在概率近似正确(PAC)框架下，一个概念(类)，如果存在一个多项式的学习算法能够学习它，并且正确率很高，就称这个概念是强可学习的
> **弱可学习：** 一个概念，如果存在一个多项式的算法能够学习它，但学习的正确率仅仅比随机猜测好，则称该概念是弱可学习

后来Schapire证明了一个重要结论:
> **一个概念是强可学习的  $\Leftrightarrow$ 一个概念是弱可学习的**

这样一来，问题1就转换成了另一个问题:
> **问题2: 在学习中国，如果已经发现了“弱可学习算法”，那么能否将它提升为强可学习算法？**

该部分介绍以下几类提升算法:
- Bagging
- **AdaBoost**
- 提升树

<!--more-->
### Bagging 
对于一个训练数据集，如果我们将所有的训练集数据拿来训练，那么我们便只能得到一个模型，因此Bagging的思想非常简单：
> 每次从训练数据集中取一部分数据来训练模型，通过这种方式得到多个模型，最后的决策结果则由这些模型投票表决

算法流程如下:
> - Step1: 从包含$n$个样本的数据集中取$m$个样本作为新的训练集
> - Step2: 用挑选出的数据集来训练得到一个模型
> - Step3: 重复Step1 L次，得到L个模型
> - Step4: 最终决策结果由这$L$个模型多数投票得到

### AdaBoost
#### 算法流程
Bagging的思想比较简单粗暴，所有样本都一视同仁，而另一种思路则是改变训练数据的概率分布(训练数据的权值分布)，针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。这样，对于提升算法而言，有两个问题需要回答：
- 在每一轮如何改变训练数据的权值或概率分布  
- 如何将这些弱分类器组合成一个强分类器

AdaBoost针对这两个问题的的解决方案是:
- 第1个问题: 提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类的样本的权值。这样一来，那些没有得到正确分类的数据由于权值加大而受到后一轮弱分类器的更多关注。
- 第2个问题：AdaBoost采用加权表决的方法,加大分类误差率小的弱分类器的权值。

AdaBoost算法流程如下:
> **AdaBoost算法**
> **输入：** 训练数据集$T = \{ (x_1,y_1), (x_2,y_2),\dots, (x_N,y_N)\}$,其中$x_i \in R^n, y_i \in \{ -1, +1 \}$,弱学习算法
> **输出：** 最终分类器$G(x)$ 
> 1. 初始化训练数据的权值分布
> $$
    D_1 = (w_{11}, \dots, w_{1i}, \dots,w_{1N}), w_{1i} = \frac{1}{N}
> $$
> 2. 对于$m = 1,2,\dots,M$ 
>    - 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器
>    $$
        G_m(x): \mathcal{X} \rightarrow \{ -1, +1 \}
>    $$
>    - 计算$G_m(x)$ 在训练数据集上的分类误差率
>    $$
        e_m = \sum_{i=1}^N P(G_m(x_i) \neq y_i) = \sum_{i=1}^N w_{mi} I(G_m(x_i) \neq y_i)
>    $$
>    - 计算$G_m(x)$的系数:
>    $$
        \alpha_m = \frac{1}{2} \log \frac{1 - e_m}{e_m}
>    $$
>   - 更新训练数据集的权值分布
>   $$
    \begin{aligned}
        D_{m+1} &= (w_{m+1,1},\dots, w_{m+1,i}, \dots,w_{m+1,N}) \\
        w_{m+1,i} &= \frac{w_{m+1,i}}{Z_{m}} \exp(- \alpha_m y_i G_m(x_i)) \\
        Z_m &= \sum_{i=1}^N w_{mi} \exp(-\alpha_m y_i G_m(x_i))
    \end{aligned}
>   $$
>   - 构建基本分类器的线性组合:
>     $$
         f(x) = \sum_{m=1}^M \alpha_m G_m(x) 
>     $$
>   得到最终分类器：
>   $$
        G(x) = sign(f(x)) = sign(\sum_{m=1}^M \alpha_m G_m(x))
>   $$

下面对AdaBoost算法说明如下:
- 在最开始假设训练数据具有均匀的权值分布，即每个训练样本在基本分类器中的作用相同，这一假设保证第一步能够在原始数据上学习基本分类器$G_1(x)$
- AdaBoost反复学习基本分类器，在每一轮弱分类器训练中，首先使用当前分布$D_m$加权的训练数据集，学习基本分类器$G_m(x)$,使用加权数据进行训练，最终学得的分类器更加关注权重较高的数据，接下来计算该基本分类器$G_m(x)$在加权训练数据集上的分类误差率,然后通过该误差率计算分类器$G_m(x)$的权重，从权重计算公式可以看出，当$e_m \leq 0.5$(弱分类器前提假设)时，$\alpha_m \geq 0$,且随着$e_m$减小，$\alpha_m$会不断增大。 
- 接下来更新训练数据的权值，对于被分类器$G_m(x)$分类正确的样本，增加其权重，反之减少其权重。**不改变所给的训练数据，而是不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同作用。**
- 最终的组合分类器是一个基本分类器的线性组合形式，$f(x)$的符号决定了实例$x$的类，而$f(x)$的绝对值则显示了分类结果的置信度。

#### 算法理论说明
上一部分从算法思想来对AdaBoost进行理解，这一部分将从理论推导方面证明AdaBoost中的一些关键公式是如何来的，比如：
- 分类器权重$\alpha_m = \frac{1}{2} \log \frac{1-e_m}{e_m}$
- 样本点权重更新公式： 
$$
\begin{aligned}
    D_{m+1} &= (w_{m+1,1},\dots, w_{m+1,i}, \dots,w_{m+1,N}) \\
    w_{m+1,i} &= \frac{w_{m+1,i}}{Z_{m}} \exp(- \alpha_m y_i G_m(x_i)) \\
    Z_m &= \sum_{i=1}^N w_{mi} \exp(-\alpha_m y_i G_m(x_i))
\end{aligned}
$$

首先来推导下AdaBoost的训练误差界，样本$x_i$ 在第$t$轮训练时的权重，由样本点权重递推公式可以得到：
$$
    w_{ti} = w_{t-1i} \frac{e^{-\alpha_t y_i h_t(i)}}{Z_{t-1}} = \dots = w_{1i} \frac{e^{-y_i \sum_{j=1}^t \alpha_j h_j(t)}}{\prod_{i=1}^{t-1} Z_i}
$$
其中$w_{1i} = \frac{1}{N}$