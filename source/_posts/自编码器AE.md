---
title: 自编码器AE
date: 2020-10-10 20:20:27
tags: 自编码，变分自编码，降噪自编码 
categories: 统计学习 
mathjax: true 
toc: true 
---
在进行数据降维和数据降噪时常用的深度学习方法是自编码器`AutoEncoder`类算法，其具有以下特点:
- 自编码器类算法属于无监督学习范畴
- 从结构上来看自编码器类算法包含编码器和解码器两部分 
- 自编码器算法常用做特征提取 

本文按照以下结构进行组织:
- 自编码器AE
- 变分自编码器VAE 
- 降噪自编码器DAE 
<!--more-->
### 自编码器AE  

#### 自编码器算法思想 
所谓的自编码器从名称上来理解，便是对自己进行编码，首先给出维基百科上关于自编码器的定义:
> **自编码器:** 自编码，也称自动编码器，是一种人工神经网络，在无监督学习中用于有效编码。自编码的目的是对一组数据学出一种表示，通常用于降维。 

一个自编码器一般具有如下结构: 
![自编码器](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/AE.png)

从上图可以看出，自编码器模型主要由编码器和解码器组成，其主要目的是将输入变量映射到中间变量$z$，然后再将中间变量$z$映射成$x'$， 模型训练的目标是$x$和$x'$尽可能接近，编码器和解码器分别实现了以下功能: 
- 编码器: 将输入数据映射到隐含特征空间 
- 解码器: 基于隐含特征来重构原始数据 

单纯从形式上来看，自编码器就是一个普通的神经网络，只是网络输出为输入数据本身；但如果从算法思想的角度，我们一般所接触的神经网络所开展的任务往往是有监督学习问题，比如分类和回归预测，而对于无监督学习问题，比如聚类、降维，还是统计学习中的一些传统算法应用较多，而自编码器则就给予了我们一个应用深度学习方法来开展无监督学习的思路。

初学者在开展自编码器学习过程中绕不开的一个问题就是:
> 训练这样一个恒等映射网络有什么意义? 

我在刚了解自编码器算法思想时也有这样的疑问，但在对无监督学习思想有了更多理解之后我认为构建这样一个恒等映射网络有以下几方面应用: 
- **数据压缩/降维:** 在图像压缩领域，一个图像可能动辄几MB甚至几十MB，在进行传输中需要大量的流量，而假设我们已经有了一个训练好的自编码神经网络，而特征空间维度较低，我们就可以在发送端应用训练好的自编码器进行数据压缩，然后将压缩后的数据传输到接收端，接收端再使用解码器来对图像进行还原。
- **特征提取:** 数据压缩本身也是一种特征提取，而具体提取什么样的特征则是与数据相关的，而自编码器提取特征的标准则是希望提取的隐层特征能够在自编码器神经网络框架下较精确地重构原始数据，提取后的特征可以送入下游机器学习算法中以完成下游任务。

在进行自编码器算法应用时，需要注意以下两点问题: 
- 自动编码器是数据相关的，这也就意味着自动编码器只能压缩那些与训练数据类似的数据，如果从概率分布的角度来进行理解，编码器其实就是学到了一个映射，将原始空间的数据分布映射到特征空间的某个概率分布，这也就意味着如果模型训练的比较充分且新来的样本与训练样本同分布，则自编码器就能较好地对新样本数据进行编码。 
- 自动编码器是有损的，这是因为经过编码解码后的输出与输入相比还是有误差的/退化的，这个误差我们一般称之为重构误差。
- 自动编码器算法属于一种无监督学习算法。

#### 自编码器搭建及学习 

自编码器其实可以看作一整个神经网络，包含编码器与解码器两部分，分别学习不同的映射: 
$$
    \begin{aligned}
        &Encoder: x \rightarrow f(x) \quad \mathbb{R}^d \rightarrow \mathbb{R}^{d'}\\ 
        &Decoder: f(x) \rightarrow x' \quad \mathbb{R}^{d'} \rightarrow \mathbb{R}^d
    \end{aligned}
$$
如果对自编码器的算法思想理解了同时具备神经网络搭建训练方面的知识，则自编码器的搭建及训练不需要再单独学习。

#### 常见自编码器类型 
常用自编码器有以下集中几种类型: 
- **堆栈自编码器**: 所谓堆栈自编码器实际上就是最普通的自编码器，编码器和解码器网络结构层数均大于1。
- **欠完备自编码器**: 如果编码器输出特征维度$h$小于原始数据特征维度$d$，则对应的自编码器称为欠完备自编码器，在进行数据降维时一般都是采用欠完备自编码器。
- **正则自编码器**: 普通的堆栈自编码器只是学习了一个恒等映射，而我们有时候期望降维后的特征能够具有某些特性，这些特性包括系数表示、对噪声鲁棒性等，而要达成这样的目的往往会在目标函数上添加一个正则项。
- **降噪自编码器**: 降噪自编码器与其说是一个算法结构，实际上更像是一种模型训练的trick，降噪自编码器以损坏/缺失的数据作为输入，然后来预测原始的未损坏/缺失的数据，从而使自编码器学习到从缺失数据恢复为未缺失数据的能力。   
### 变分自编码器VAE 
#### 自编码器生成内容局限性 
在训练好了一个自编码器以后，我们可能会考虑这样一个问题:
> 能否应用自编码器来进行数据生成? 
> 如果隐空间足够规则，我们是否可以从隐空间中随机取一个点并将其解码以获得新的内容，就像生成对抗网络中的生成器那样。 

这样做是并不行的，主要是因为两个原因:
> - 隐空间缺乏可解释和可利用的结构，缺乏规则性( lack of regularity )，关于这一点我们可以举一个极端的例子，假设编码器和解码器足够强大，将$N$个样本分别映射到了一维的隐空间，分别对应着$1~N$。这也就导致你如果随便取隐空间一个点，然后用解码器进行解码，那么大概率不能生成有用样本。 
> - 在进行降维时，由于自编码器过于强大，导致很多原始空间中数据的结构信息并没有得到保留，在隐空间中是稀疏的。 

换句话说，普通的自编码器并不是生成式模型，其更像是学习了某种确定性映射，并不能用于生成新的样本。

#### 变分自编码器思想
为了能够能够使得自编码器的解码器用于生成目的，我们需要保证隐空间足够规则，获得这种规律性的一种可能方案是在训练过程中引入显示的正则项。**因此，变分自编码器可以看作是一种自编码器，其训练过程经过正则化以避免过拟合，同时使隐变量$z$分布服从某一特定分布，在进行数据生成时便可以从该分布中采样然后送入解码器以进行数据生成。**

变分自编码器与普通自编码器在形式上的不同点在于:
> 某个输入变量$x$并不是被编码成隐空间中一个点，而是隐空间中的一个概率分布。 

$$
    \begin{aligned}
        &AE: x \rightarrow Encoder: z = f(x) \rightarrow Decoder: x' = g(z) \\
        &VAE: x \rightarrow Encoder: z \sim N(x|\mu, \sigma^2) \rightarrow Sample \ z  \rightarrow Decoder: x' = g(z)
    \end{aligned} 
$$

我们一般假设隐空间中隐变量的分布是高斯分布，而编码器实际上就是学习对应高斯分布的均值和方差,整个变分自编码器的结构如下图所示:
![VAE](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/VAE.png) 
需要注意的是，由多少个样本，则最终在隐空间就会有多少个高斯分布。因为在进行解码之前有一个采样的操作，而采样本身是并不可导的，因此在实际中都是采用一个叫做`重参数`的技巧，也就是说每一次都是从标准正态分布中采样，然后通过:
$$
    \begin{aligned}
        z &= \mu + \sigma \odot \epsilon  \\
        \epsilon &\sim N(0,1)
    \end{aligned}
$$
来重构隐变量，这样$\mu$和$\sigma$就变成了一个参数，是可以应用反向传播进行参数更新。
![VAE](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/VAE1.png)
在进行模型训练时，主要是以下步骤:
- 将输入编码为隐空间上一个高斯分布(均值$\mu$, 方差$\sigma^2$) 
- 从该分布中采样隐空间中一个点
- 对采样点进行解码并计算出重构误差 
- 重构误差通过网络进行反向传播

而实现隐空间正态规则化最简单的实现方式便是在目标函数中添加正则项，该正则项期望输入映射到隐空间的这个正态分布能够尽可能接近标准正态分布，度量方式一般选用KL散度，因此最终目标函数一般是如下形式:
$$
    loss = ||x - x'||^2 + D_{KL}(N(\mu_x, \sigma_x^2) || N(0,1))
$$

#### 关于正则化直观解释 
为了使生成过程成为可能，我们期望隐空间具有规则性，而规则性可以通过两个主要属性表示: 
- 连续性: 隐空间中的两个相邻点解码后不应呈现两个完全不同的内容 
- 完整性: 针对给定的分布，从隐空间采样的点在解码后应提供“有意义”的内容 

VAE将输入编码为分布而不是点并不足以确保连续性和完整性，如果没有明确地定义正则化项，即模型目标函数仅仅期望重构误差尽可能小，那么模型会倾向于让分布的方差等于0以获得更小的重构误差，而此时变分自编码器也就退化成了普通的自编码器。

因此，为了避免该问题，我们必须同时对协方差矩阵和编码器返回的分布均值进行正则化，在实际应用中则是通过计算投影分布与标准正态分布之间的KL散度作为正则项。 

使用该正则化项，可以防止模型在隐空间中的编码相互远离，并鼓励尽可能多的返回分布发生重叠，从而满足预期的连续性和完整性条件。自然的对于任何正则化项，都会以训练数据上更高的重建误差为代价。然而，可以调整重建误差和KL散度之间的权重，我们将在下一节中看到如何从形式推导中自然得出平衡的表达。

#### 数学推导 
##### 概率框架和假设 
我们首先定义一个概率图模型来描述我们的数据，用$x$表示我们的数据变量，并且假定$x$是由未直接观察到的潜在变量$z$生成。因此对于每个数据点，假设以以下两个步骤进行生成:
- 首先，从先验分布p(z)中采样一个隐空间表示$z$
- 第二，按条件概率$p(x|z)$采集数据$x$
$$
    z \rightarrow x 
$$

在这种概率模型下，我们可以重新定义编码器和解码器的概念。实际上，与考虑使用确定性编码器和解码器的简单自编码器不同，我们现在将考虑这两个对象的概率版本。自然地，“概率解码器”由$p(x|z)$定义，描述由给定已编码变量到解码变量的分布，而“概率编码器”由$p(x|z)$定义，描述根据原始变量给出编码变量的分布。

我们假设隐空间中编码变量$z$遵循先验分布$p(z)$,则由贝叶斯公式，编码条件概率分布可以表示为:
$$
    p(z|x) = \frac{p(x|z) p(z)}{p(x)} = \frac{p(x|z)p(z)}{\int_z p(x|z) p(z) dz}
$$
我们假设$p(z)$是标准高斯分布，而$p(x|z)$是高斯分布，其均值由变量$z$的确定性函数$f$定义，并且协方差矩阵定义为$\Sigma = cI$,因此我们有:
$$
    \begin{aligned}
        &z \sim N(0,1)  \\
        &x|z \sim N(f(z), cI),f \in F, c>0 
    \end{aligned}
$$
从理论上讲，我们知道$p(z)$和$p(x|z)$，便可以使用贝叶斯定理来进行计算$p(z|x)$,但是这种计算由于有分母的积分存在，直接计算并不容易，因此需要采用变分推断技术来近似获得$p(x)$。 

##### 变分推断技术 
首先给出变分贝叶斯方法的维基百科定义:
> **变分贝叶斯方法**是一类用于近似贝叶斯推理和机器学习中产生的棘手积分的技术。它们通常用于由观察变量以及未知参数和潜在变量组成的复杂统计模型中，如图模型描述，这三种类型的随机变量之间具有各种关系。变分贝叶斯方法主要用于两个目的:
> - 为了对未观察变量的为了对未观察变量的后验概率提供解析近似，以便对这些变量进行统计推断。
> - 得出观察数据的边际可能性（有时称为“证据”）的下限（即，给定模型的数据的边际概率，对未观察变量进行边际化）。这通常用于执行模型选择，通常的想法是，给定模型的边际可能性越高，表明该模型对数据的拟合度越高，因此，所讨论的模型是生成数据的模型的可能性就越大。（另请参阅贝叶斯因子文章。）

变分推断是一种近似复杂分布的技术，基本的思路便是设置一个参数化的分布，并在该族中寻找目标分布的最佳近似，分布差异之间的度量一般采用[KL散度](https://soundofwind.top/2020/08/26/gai-lu-fen-bu-zhong-de-ju-chi-du-liang/)来作为分布之间的度量指标。在这里，我们将通过高斯分布$q_x(z)$来近似后验概率$p(z|x)$，我们可以写出如下表示:
$$
    q_x(z) = N(g(x), h(x)), g \in G, h \in H 
$$
我们期待这个合适的参数$g(x), h(x)$以尽可能缩小两个变量之间的KL散度: 
$$
    \begin{aligned}
        (g^\ast, h^\ast) &= \arg \min_{g,h} D_{KL}(q_x(z), p(z|x)) \\
        &= \arg \min_{g,h} E_{z \sim q_x} (log q_x(z)) - E_{z \sim q_x } (log \frac{p(x|z) p(z)}{p(x)}) \\ 
        &= \arg \min_{g,h} E_{z \sim q_x} (log q_x(z))  - E_{z \sim q(x)} (log p(z)) - E_{z \sim q_x} (log p(x|z)) + E_{z \sim q_x} (log p(x)) \\
        &= \arg \max_{g,h} E_{z \sim q_x} (log p(x|z)) - KL(q_x(z), p(z)) \\ 
        &= \arg \max_{g,h} E_{z \sim q_x} (-\frac{||x-f(z)||^2}{2c}) - KL(q_x(z), p(z))
    \end{aligned}
$$
从最终优化目标的表达式来看，一方面期望最大化重构出的样本的对数似然最大，另一方面期望$q_x(z)$接近先验分布$p(z)$。这种折衷体现了在贝叶斯推理中，在数据置信度与先验分布置信度之间的平衡。
##### 条件变分自编码器CVAE 
用VAE生成数据的一个问题是，我们对于生成的数据没有任何控制。例如, 如果我们用MNIST数据集训练VAE，并尝试通过相解码器输入$Z \sim N(0,1)$来生成图像，它也会产生不同的随机数字图像，但我们并不能控制究竟生成哪一个数字的图像，而我们拿到的数据是有着标签信息，因此我们期望能够将标签信息也融入到训练过程当中，以使编码器具有生成任意标签数据的能力。 
为此，我们需要对VAE体系进行修改，假设给定一个输入标签$Y$，我们希望生成模型能够输出该标签对应的图像，那么可以在普通自编码器基础上做如下修改，在进行编码时同时加上标签信息，然后在进行解码时也加上标签信息，整个生成过如下:
$$
    y,x \rightarrow z,y \rightarrow x 
$$
在这里编码器学习后验概率分布$p(z|x,y)$实际上相当于是将标签信息从隐空间分离表征，然后在进行解码时再加入标签信息，表明需要生成哪一类的样本。
![CVAE](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/CVAE.png)

##### 总结 
到目前为止，如果假设函数$f$是已知且固定的，则我们可以使用变分推理技术来近似后验$p(z|x)$。但是实际上，定义解码器的函数$f$是未知的，而且也需要求解。但我们最初的目标是找到一种性能良好的编码/解码方案，其隐空间又足够规则，可以用于生成目的。如果规则性主要由在隐空间上假定的先验分布所决定，则整个编码-解码方案的性能高度取决于函数$f$的选择。确实，由于$p(z|x)$可以从$p(z)$和p(x|z)近似（通过变分推论），而$p(z)$是简单的标准高斯模型，仅存的两个需要我们优化的对象是参数$c$(决定了分布的协方差)和函数$f$(决定了分布的均值)。

对于$F$中的任意一个函数$f$都定义了一个概率解码器(p(x|z)),然后可以得到$p(z|x)$的最佳近似$q^\ast_x(z)$,然后当给定从$q_x^\ast(z)$中的一次采样$z$时，我们期望选择函数$f$使$x$的期望对数似然概率最大。**换句话说，对于给定的输入$x$,当我们从分布$q_x^\ast(z)$采样$z$然后从分布$p(x|z)$采样$\hat{x}$时，我们希望寻找到合适的参数$f$，使得 $x = \hat{x}$的概率最大。**

将上面所有部分总在一起，我们正在寻找最优的$f^\ast, h^\ast, g^\ast$，使得:
$$
\left(f^{*}, g^{*}, h^{*}\right)=\arg \max _{(f, g, h) \in F \times G \times H}\left(\mathbb{E}_{z \sim q_{x}}\left(-\frac{\|x-f(z)\|^{2}}{2 c}-K L\left(q_{x}(z), p(z)\right)\right)\right)
$$  
常数$c$决定了两个条件之间的平衡，$c$越高，则表示我们对重构误差的容忍度越大，更加关注正则化项，如果$c$低，则相反。

#### 将神经网络引入模型 

到目前为止，我们已经建立了一个依赖于三个函数$f, g$和$h$的概率模型，我们无法轻松地在整个函数空间上进行优化，因此我们限制了优化域，并将$f,g,h$用神经网络来定义。因此，$F,G,H$分别对应于网络体系结构上的参数族。

实际上，$g$和$h$不是由两个完全独立的网络定义的，而是共享它们一部分结构和参数，因此我们可以:
$$
    g(x) = g_2(g_1(x)),h(x) = h_2(h_1(x)),g_1(x) = h_1(x) 
$$
解码器则是一般的神经网络结构，这样就获得了整体的网络结构，可以进行优化。
![VAE](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/beta-VAE.png) 

### 正则自编码器 
变分自编码器也可以看作是一种特殊的正则自编码器, 这部分主要介绍下降噪自编码器和稀疏自编码器。 
#### 降噪自编码器DAE
降噪自编码的核心思想是，建立一个恒等映射的自编码器未必是好的，能够将“损坏”的数据还原成“好”的数据的自编码器才是好的。
![DAE](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/Denoising-Autoencoder_qm5AOQM.png) 
因此网络输入往往会对输入$x$做加噪处理$\hat{x}$，然后经过自编码器重构未加噪的输入$x$,这样训练得到的自编码器可以获得一定的对于原始数据波动的鲁棒性。

#### 稀疏自编码器 
我们有时期望能够获得输入数据的高维稀疏表示，则需要对加入一个正则项来实现这个目的，稀疏自编码器一般是一个三层的神经网络，其目标函数包含两部分，一部分是重构误差，另一部分则是稀疏正则项。
![稀疏自编码](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/xishu.jpg)
若输入样本个数为$m$个，隐层神经元个数为$s_2$个，在这里首先定义隐层神经元$j$对于所有训练数据的平均激活度:
$$
    \hat{\rho}_j = \frac{1}{m} \sum_{i=1}^m [a_j^{(2)} (x^{(i)})] 
$$
其中$a_j^{(2)}(x^{(i)})$为隐层第$j$个神经元对第$i$个样本的响应输出，然后我们会设置一个稀疏系数$\rho = 0.05/0.1$,通过KL散度来度量$\rho, \hat{\rho}_j$之间的差异:
$$
    D_{KL}(\rho || \hat{\rho}_j) = \rho log \frac{\rho}{\hat{\rho}_j} + (1-\rho) log \frac{1-\rho}{1-\hat{\rho}_j}
$$
因为有$s_2$个隐层神经元，因此最终稀疏正则项表达式为:
$$
    sparse_{loss} = \sum_{j=1}^{s_2} D_{KL} (\rho || \hat{\rho}_j)
$$
通过这样的一项稀疏正则化，最终训练得到的编码器将会把输入$x$编码为高维空间的稀疏表达，有助于提取更具意义的边缘特征。

可以这样简单进行理解，使用普通的自编码器对图像进行压缩/编码是在像素级别上的编码，而对于图像而言，像素层次上的编码往往并不具备意义，通过加入稀疏正则化项，网络会更加关注一些high level的特征，比如眼睛是大是小，嘴巴形状等等，而不仅仅是拘泥于像素层级的特征提取。 





