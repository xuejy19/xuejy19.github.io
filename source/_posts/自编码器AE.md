---
title: 自编码器AE
date: 2020-10-10 20:20:27
tags: 自编码，变分自编码，降噪自编码 
categories: 统计学习 
mathjax: true 
toc: true 
---
在进行数据降维和数据降噪时常用的深度学习方法是自编码器`AutoEncoder`类算法，其具有以下特点:
- 自编码器类算法属于无监督学习范畴
- 从结构上来看自编码器类算法包含编码器和解码器两部分 
- 自编码器算法常用做特征提取 

本文按照以下结构进行组织:
- 自编码器AE
- 变分自编码器VAE 
- 降噪自编码器DAE 
<!--more-->
### 自编码器AE  

#### 自编码器算法思想 
所谓的自编码器从名称上来理解，便是对自己进行编码，首先给出维基百科上关于自编码器的定义:
> **自编码器:** 自编码，也称自动编码器，是一种人工神经网络，在无监督学习中用于有效编码。自编码的目的是对一组数据学出一种表示，通常用于降维。 

一个自编码器一般具有如下结构: 
![自编码器](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/AE.png)

从上图可以看出，自编码器模型主要由编码器和解码器组成，其主要目的是将输入变量映射到中间变量$z$，然后再将中间变量$z$映射成$x'$， 模型训练的目标是$x$和$x'$尽可能接近，编码器和解码器分别实现了以下功能: 
- 编码器: 将输入数据映射到隐含特征空间 
- 解码器: 基于隐含特征来重构原始数据 

单纯从形式上来看，自编码器就是一个普通的神经网络，只是网络输出为输入数据本身；但如果从算法思想的角度，我们一般所接触的神经网络所开展的任务往往是有监督学习问题，比如分类和回归预测，而对于无监督学习问题，比如聚类、降维，还是统计学习中的一些传统算法应用较多，而自编码器则就给予了我们一个应用深度学习方法来开展无监督学习的思路。

初学者在开展自编码器学习过程中绕不开的一个问题就是:
> 训练这样一个恒等映射网络有什么意义? 

我在刚了解自编码器算法思想时也有这样的疑问，但在对无监督学习思想有了更多理解之后我认为构建这样一个恒等映射网络有以下几方面应用: 
- **数据压缩/降维:** 在图像压缩领域，一个图像可能动辄几MB甚至几十MB，在进行传输中需要大量的流量，而假设我们已经有了一个训练好的自编码神经网络，而特征空间维度较低，我们就可以在发送端应用训练好的自编码器进行数据压缩，然后将压缩后的数据传输到接收端，接收端再使用解码器来对图像进行还原。
- **特征提取:** 数据压缩本身也是一种特征提取，而具体提取什么样的特征则是与数据相关的，而自编码器提取特征的标准则是希望提取的隐层特征能够在自编码器神经网络框架下较精确地重构原始数据，提取后的特征可以送入下游机器学习算法中以完成下游任务。

在进行自编码器算法应用时，需要注意以下两点问题: 
- 自动编码器是数据相关的，这也就意味着自动编码器只能压缩那些与训练数据类似的数据，如果从概率分布的角度来进行理解，编码器其实就是学到了一个映射，将原始空间的数据分布映射到特征空间的某个概率分布，这也就意味着如果模型训练的比较充分且新来的样本与训练样本同分布，则自编码器就能较好地对新样本数据进行编码。 
- 自动编码器是有损的，这是因为经过编码解码后的输出与输入相比还是有误差的/退化的，这个误差我们一般称之为重构误差。
- 自动编码器算法属于一种无监督学习算法。

#### 自编码器搭建及学习 

自编码器其实可以看作一整个神经网络，包含编码器与解码器两部分，分别学习不同的映射: 
$$
    \begin{aligned}
        &Encoder: x \rightarrow f(x) \quad \mathbb{R}^d \rightarrow \mathbb{R}^{d'}\\ 
        &Decoder: f(x) \rightarrow x' \quad \mathbb{R}^{d'} \rightarrow \mathbb{R}^d
    \end{aligned}
$$
如果对自编码器的算法思想理解了同时具备神经网络搭建训练方面的知识，则自编码器的搭建及训练不需要再单独学习。

#### 常见自编码器类型 
常用自编码器有以下集中几种类型: 
- **堆栈自编码器**: 所谓堆栈自编码器实际上就是最普通的自编码器，编码器和解码器网络结构层数均大于1。
- **欠完备自编码器**: 如果编码器输出特征维度$h$小于原始数据特征维度$d$，则对应的自编码器称为欠完备自编码器，在进行数据降维时一般都是采用欠完备自编码器。
- **正则自编码器**: 普通的堆栈自编码器只是学习了一个恒等映射，而我们有时候期望降维后的特征能够具有某些特性，这些特性包括系数表示、对噪声鲁棒性等，而要达成这样的目的往往会在目标函数上添加一个正则项。
- **降噪自编码器**: 降噪自编码器与其说是一个算法结构，实际上更像是一种模型训练的trick，降噪自编码器以损坏/缺失的数据作为输入，然后来预测原始的未损坏/缺失的数据，从而使自编码器学习到从缺失数据恢复为未缺失数据的能力。   
### 变分自编码器VAE 
#### 自编码器生成内容局限性 
在训练好了一个自编码器以后，我们可能会考虑这样一个问题:
> 能否应用自编码器来进行数据生成? 
> 如果隐空间足够规则，我们是否可以从隐空间中随机取一个点并将其解码以获得新的内容，就像生成对抗网络中的生成器那样。 

这样做是并不行的，主要是因为两个原因:
> - 隐空间缺乏可解释和可利用的结构，缺乏规则性( lack of regularity )，关于这一点我们可以举一个极端的例子，假设编码器和解码器足够强大，将$N$个样本分别映射到了一维的隐空间，分别对应着$1~N$。这也就导致你如果随便取隐空间一个点，然后用解码器进行解码，那么大概率不能生成有用样本。 
> - 在进行降维时，由于自编码器过于强大，导致很多原始空间中数据的结构信息并没有得到保留，在隐空间中是稀疏的。 

换句话说，普通的自编码器并不是生成式模型，其更像是学习了某种确定性映射，并不能用于生成新的样本。

#### 变分自编码器思想
为了能够能够使得自编码器的解码器用于生成目的，我们需要保证隐空间足够规则，获得这种规律性的一种可能方案是在训练过程中引入显示的正则项。**因此，变分自编码器可以看作是一种自编码器，其训练过程经过正则化以避免过拟合，同时使隐变量$z$分布服从某一特定分布，在进行数据生成时便可以从该分布中采样然后送入解码器以进行数据生成。**

变分自编码器与普通自编码器在形式上的不同点在于:
> 某个输入变量$x$并不是被编码成隐空间中一个点，而是隐空间中的一个概率分布。 

$$
    \begin{aligned}
        &AE: x \rightarrow Encoder: z = f(x) \rightarrow Decoder: x' = g(z) \\
        &VAE: x \rightarrow Encoder: z \sim N(x|\mu, \sigma^2) \rightarrow Sample \ z  \rightarrow Decoder: x' = g(z)
    \end{aligned} 
$$

我们一般假设隐空间中隐变量的分布是高斯分布，而编码器实际上就是学习对应高斯分布的均值和方差,整个变分自编码器的结构如下图所示:
![VAE](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/VAE.png) 
需要注意的是，由多少个样本，则最终在隐空间就会有多少个高斯分布。因为在进行解码之前有一个采样的操作，而采样本身是并不可导的，因此在实际中都是采用一个叫做`重参数`的技巧，也就是说每一次都是从标准正态分布中采样，然后通过:
$$
    \begin{aligned}
        z &= \mu + \sigma \odot \epsilon  \\
        \epsilon &\sim N(0,1)
    \end{aligned}
$$
来重构隐变量，这样$\mu$和$\sigma$就变成了一个参数，是可以应用反向传播进行参数更新。
