---
title: 生成对抗网络GAN
date: 2020-09-09 23:34:49
tags: GAN
categories: 统计学习 
mathjax: true
toc: true
---
这一部分介绍一种特殊的神经网路模型——生成对抗网络(GAN)，生成对抗网络由[Lan Goodfellow](https://en.wikipedia.org/wiki/Ian_Goodfellow)于2014年提出，该算法在形式上表现为两个神经网络的彼此对抗，对于生成对抗网络，我们可以从以下几个角度来对其进行限定：
- **本质：** 学习训练数据集的分布  
- **作用：** 产生新的样本，对于小样本任务可以起到数据增强的作用 
- **实现形式：** 两个神经网络彼此对抗 
 
本文按照以下结构进行组织:
- GAN算法思想
- GAN背后数学推导  
- GAN`pytorch`实现 
  
<!--more-->

### GAN 算法思想
![GAN](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/GAN.jpg)
考虑这样一个场景，一个小镇里有一个造假钞的人，同时有一个警察，它们两个各自的目标分别是：
- **罪犯：** 不断提高自己的造假钞技术，使得自己的假钞足以以假乱真，让警察鉴别不出来
- **警察：** 不断提高自己的鉴别水平，能够准确地识别小偷制造的假钞

从博弈论的角度来看，这其实是一个零和博弈：
> 小偷的造假技术越高超，则警察鉴别起来就越困难;反之，警察的鉴别技术越高超，则小偷造出能欺骗过警察的假钞就更加困难，在对抗中，两个人都在变强。

接下来我们分析构想一个场景，我们有一组数据，但是数据的量不足，我们期望能够找到这样一个"造假数据"的人帮助我们产生一些新的图像，以达到数据增强的目的。但在这个"造假数据"的人造出一个假数据之后，我们需要判断一下这个数据到底合不合格，因此我们就需要另外一个鉴别"假数据"的"警察"。 

其实我们的本质目的是为了**造假数据**,但这个任务若无监督是很难进行下去的，因此我们需要为其提供一个"警察"对其进行监督，因此我们就需要有两个学习器，一个学习如何生成"假数据"，另一个则需要学习如何判别出这些"假数据"，在这两个学习器博弈的过程中我们最终得到了一个"造假数据"比较高超学习器。

### GAN 背后数学推导 
> **沃兹基**曾经说过：算法思想也就图一乐，真涨姿势还是要看数学推导

#### 从最大似然估计谈起
因为我们本质上是希望能够得到一个学习器，使得其产生的数据的能够与训练集数据同分布，那么数学推导就从分布入手，假设训练集数据概率分布为$p(x|\theta)$,其中$\theta$为该概率分布所依赖的参数，当我们得到一组数据$X = (x_1,x_2,\dots, x_N)$时，我们想要对参数进行估计，那么此时就要祭出已经老生常谈的**最大似然估计**，假设样本独立同分布，则写出似然函数如下:
$$
    L(X|\theta) = \prod_{i=1}^N p(x_i | \theta)
$$
而$\theta$的求解其实就是一个优化问题:
$$
    \begin{aligned}
        \theta^* &= \argmax L(X|\theta) \\
        &= \argmax \ln L(X|\theta) \\  
        &= \argmax \frac{1}{N} L(X|\theta) \\
        &= \argmax \frac{1}{N} \sum_{i=1}^N \ln p(x_i|\theta) \\
        &= \argmax E_{x \sim p(x)} \ln p(x|\theta)  \\
        &= \argmax \int_{x} p(x) \ln p(x|\theta) dx
    \end{aligned}
$$
这里需要特别说明一下这一步：
$$
    \argmax \frac{1}{N} \sum_{i=1}^N \ln p(x_i|\theta) \Leftrightarrow \argmax E_{x \sim p(x)} \ln p(x|\theta)
$$
当样本数量足够大时，空间中每个样本点都被覆盖，而具体空间中某一个点$x_i$会落入多少样本点则取决于数据分布$p(x)$在该点概率密度函数的大小，换句话说，如果从采样的角度，要对某点的概率密度函数进行估计，那么只需要原始数据进行采样，采样$N$个点，若最终有$n_{x_i}$个点落在了$x_i$处，那么该点的概率密度函数就可以估计为:
$$
    p(x_i) = \frac{n_{x_i}}{N}
$$
于是上面的等价性也是可以按照这种思想推导出来：
$$
    \begin{aligned}
        \lim_{N \rightarrow \infty} \frac{1}{N} \ln p(x_i|\theta) &= \sum_{x} \frac{n_{x}}{N} \ln p(x_i|\theta)  \\
        &= \int_x p(x) \ln p(x|\theta)  dx  \\
        &= E_{x \sim p(x)} \ln p(x|\theta)  
    \end{aligned}
$$
最大似然函数估计的目的是找到一组最合适的参数$\theta$使得分布$p(x|\theta)$更加符合数据分布，换句话说，就是找到一个分布$q(x)$,使得$q(x)$能够尽可能接近原始数据分布$p_{data}(x)$。 至此我们证明了：
$$
    \theta^* = \argmax L(X|\theta)  \Leftrightarrow   E_{x \sim p(x)} \ln p(x|\theta)
$$