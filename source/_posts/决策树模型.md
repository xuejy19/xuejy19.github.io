---
title: 决策树模型
date: 2020-08-30 15:26:51
tag: 树模型,categorical变量
categories: 统计学习 
mathjax: true 
toc: true 
---
这一部分介绍下决策树算法，决策树是一种基本的分类与回归表示方法，决策树模型呈树形结构，在分类问题中，可以表示为基于特征对实例进行分类的过程。决策树可以认为是"if-then"规则的集合，也可以认为是定义在特征空间与类空间上的概率分分布。本文按照以下四部分进行组织:
- 决策树模型与学习
- 特征选择
- 决策树生成算法
- 决策树剪枝算法

<!--more-->
### 决策树模型与学习
#### 决策树模型定义
首先给出决策树的定义:
> **决策树：** 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点和有向边组成，节点由两种类型：内部节点和叶节点。内部节点表示一个特征或属性，叶节点表示一个类。

一个决策树模型如下图所示：
![决策树](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/Decision_tree.png)
决策树的逻辑其实与人思考的逻辑非常贴合,都是通过一连串的"if-then"语句来进行判断，这也是决策树算法具有较好可解释性的原因。
#### 决策树与"if-then"规则
一个决策树可以转换成一组"if-then"规则,将决策树转换成"if-then"规则集合的步骤如下:由决策树的根节点到叶节点的每一条路径构建一条规则：路径上内部节点的特征对应着规则的条件，而叶节点的类则对应着规则的结论。
决策树对应的"if-then"规则集合具有一个重要的性质：互斥并且完备，即每一个实例都被一条路径或一条规则所覆盖，而且只被一条规则或一条规则所覆盖。

#### 决策树与条件概率分布
决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分上。将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元，决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。
假设$X$为表示特征的随机变量，$Y$为表示类别的随机变量，那么这个条件概率分布可以表示为$P(Y|X)$,$X$取值于给定划分下单元的集合，$Y$取值于类的集合。

#### 决策树学习 
假设给定训练数据集:
$$
    D = \{ (x_1,y_1),(x_2,y_2), \dots, (x_N,y_N)\}
$$
其中$x_i \in R^n$为特征向量,$n$为特征个数，$y_i \in \{1,2, \dots, K\}$为类标记，$N$为样本容量。
**决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。**

决策树学习本质上是从训练数据集中归纳出一组分类规则，这点与前面章节介绍过的规则学习非常类似，不过决策树所针对的问题更加具体。与训练数据集不相矛盾的决策树可能有多个，也可能一个也没有。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。从条件概率分布的角度来看，决策树学习是由训练数据集估计条件概率模型，基于特征空间划分的类的条件概率模型有无穷多个，我们选择的条件概率模型应该不仅对训练数据有很好的拟合，同时也应当对未知的数据有较好的预测。
从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优问题，这样得到的决策树是次最优的。
决策树学习算法通常是一个递归地选取最优特征，并根据该特征对训练数据进行分割，使得各数据集有一个最好的分类的过程。这一过程对应着特征空间的划分，也对应着决策树的构建。构建流程如下:
- 构建根节点，将所有训练数据都放在跟节点
- 选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。
- 判断子集中的实例是否都被正确分类，若都已经被正确分类，则构建叶节点
- 若子集不能被正确分类，则返回第二步，选取最优特征并划分子集。

通过上述流程所构建的决策树往往对训练数据有着良好的分类能力，但对未知的测试数据却未必有很好的分类能力，即可能发生过拟合现象，因此往往需要的生成的决策树自下而上进行剪枝。

### 特征选择
