---
title: 决策树模型
date: 2020-08-30 15:26:51
tag: 树模型,categorical变量
categories: 统计学习 
mathjax: true 
toc: true 
---
这一部分介绍下决策树算法，决策树是一种基本的分类与回归表示方法，决策树模型呈树形结构，在分类问题中，可以表示为基于特征对实例进行分类的过程。决策树可以认为是"if-then"规则的集合，也可以认为是定义在特征空间与类空间上的概率分分布。本文按照以下四部分进行组织:
- 决策树模型与学习
- 特征选择
- 决策树生成算法
- 决策树剪枝算法

<!--more-->
### 决策树模型与学习
#### 决策树模型定义
首先给出决策树的定义:
> **决策树：** 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点和有向边组成，节点由两种类型：内部节点和叶节点。内部节点表示一个特征或属性，叶节点表示一个类。

一个决策树模型如下图所示：
![决策树](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/Decision_tree.png)
决策树的逻辑其实与人思考的逻辑非常贴合,都是通过一连串的"if-then"语句来进行判断，这也是决策树算法具有较好可解释性的原因。
#### 决策树与"if-then"规则
一个决策树可以转换成一组"if-then"规则,将决策树转换成"if-then"规则集合的步骤如下:由决策树的根节点到叶节点的每一条路径构建一条规则：路径上内部节点的特征对应着规则的条件，而叶节点的类则对应着规则的结论。
决策树对应的"if-then"规则集合具有一个重要的性质：互斥并且完备，即每一个实例都被一条路径或一条规则所覆盖，而且只被一条规则或一条规则所覆盖。

#### 决策树与条件概率分布
决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分上。将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元，决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。
假设$X$为表示特征的随机变量，$Y$为表示类别的随机变量，那么这个条件概率分布可以表示为$P(Y|X)$,$X$取值于给定划分下单元的集合，$Y$取值于类的集合。

#### 决策树学习 
假设给定训练数据集:
$$
    D = \{ (x_1,y_1),(x_2,y_2), \dots, (x_N,y_N)\}
$$
其中$x_i \in R^n$为特征向量,$n$为特征个数，$y_i \in \{1,2, \dots, K\}$为类标记，$N$为样本容量。
**决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。**

决策树学习本质上是从训练数据集中归纳出一组分类规则，这点与前面章节介绍过的规则学习非常类似，不过决策树所针对的问题更加具体。与训练数据集不相矛盾的决策树可能有多个，也可能一个也没有。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。从条件概率分布的角度来看，决策树学习是由训练数据集估计条件概率模型，基于特征空间划分的类的条件概率模型有无穷多个，我们选择的条件概率模型应该不仅对训练数据有很好的拟合，同时也应当对未知的数据有较好的预测。
从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优问题，这样得到的决策树是次最优的。
决策树学习算法通常是一个递归地选取最优特征，并根据该特征对训练数据进行分割，使得各数据集有一个最好的分类的过程。这一过程对应着特征空间的划分，也对应着决策树的构建。构建流程如下:
- 构建根节点，将所有训练数据都放在跟节点
- 选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。
- 判断子集中的实例是否都被正确分类，若都已经被正确分类，则构建叶节点
- 若子集不能被正确分类，则返回第二步，选取最优特征并划分子集。

通过上述流程所构建的决策树往往对训练数据有着良好的分类能力，但对未知的测试数据却未必有很好的分类能力，即可能发生过拟合现象，因此往往需要的生成的决策树自下而上进行剪枝。

### 特征选择
特征选择的目的在于选取具有分类能力的特征，决策树的每一次分叉都是进行特征选择的过程, 所以有一个亟需回答的问题就是：
> **应当以什么样的标准来评判一个特征的分类能力？**

通常评价特征分类能力的指标是信息增益和信息增益比，在前面章节中我已经给出了熵和条件熵的定义：
> **熵：** 在信息论与概率统计中，熵是表示**随机变量不确定性**的度量，设$X$是一个取有限个值的离散随机变量，其概率分布为:
> $$
    P(X = x_i) = p_i, \quad i =1,2,\dots,n
> $$
则随机变量$X$的熵定义为:
$$
    H(X) = - \sum_{i=1}^n p_i \log p_i 
$$

> **条件熵：** 设有随机变量$(X,Y)$,其联合概率分布为：
$$
    P(X = x_i,Y = y_i) = p_{ij}\quad i = 1,2,\dots,n;\quad j = 1,2,\dots,m
$$
条件熵$H(Y|X)$表示在已知随机变量$X$条件下随机变量$Y$的不确定性。随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$,定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望：
$$
    H(Y|X) = \sum_{i=1}^n p_i H(Y|X = x_i)
$$
当熵和条件熵中的概率由数据估计得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵

#### 信息增益 
下面给出信息增益的定义：
> **信息增益：** 特征$A$对训练集$D$的信息增益$g(D,A)$,定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即：
>$$
    g(D,A) = H(D) - H(D|A)
>$$

给定数据集$D$和特征$A$,经验熵$H(D)$表示对数据集$D$进行分类的不确定性，而经验条件熵$H(D|A)$则表示在特征$A$给定条件下对数据集$D$进行分类的不确定性。而它们之间的差则表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。由此便可得通过信息增益来选择特征的算法如下:
> **信息增益算法**
> **输入：** 训练数据集$D$及特征$A$
> **输出：** 特征$A$对训练数据集$D$的信息增益$g(D,A)$
> - Step1: 计算数据集$D$的经验熵：
> $$
    H(D) = -\sum_{k=1}^K \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|}
> $$
> - Step2: 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$
$$
    H(D|A) = \sum_{i=1}^n \frac{|D_i|}{|D|} H(D_i) = -\sum_{i=1}^n \frac{|D_i|}{|D|} \sum_{k=1}^K \frac{|D_{ik}|}{|D_i|} \log_2 \frac{|D_{ik}|}{|D_i|}
$$
> - Step3: 计算信息增益
> $$
    g(D,A) = H(D) - H(D|A)
> $$

#### 信息增益比
以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比则可以对这一问题进行校正，下面给出信息增益比的定义:
> **信息增益比：** 特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比，即：
> $$
    g_R(D,A) = \frac{g(D,A)}{H_A(D)}
> $$
> 其中$H_A(D) = -\sum_{i=1}^n \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}$,$n$是特征$A$取值的个数