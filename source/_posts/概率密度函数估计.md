---
title: 概率密度函数估计
date: 2020-07-27 17:10:26
tags:
categories: 统计学习
toc: true 
mathjax: true
---
在使用贝叶斯理论进行决策时，我们需要知道先验概率$P(\omega)$,类条件概率密度函数$p(x|\omega)$,而在这一部分，我们便讨论下如何从样本集中将这些所需的概率密度函数估计出来<!--more-->，本文主要讨论以下几个问题：
- 如何利用样本集估算$p(x|\omega)$和$P(\omega)$
- 估计量性质
- 利用样本集估算错误率

从样本集推断总体概率分布的方法可以分为以下几种类型:
- 监督参数估计——样本所属的类别以及总体概率密度函数的形式为已知，而概率密度函数的某些参数是未知的
- 非监督参数估计——已知总体概率密度函数形式，但未知样本所属种类，要求推断出概率密度函数某些参数
- 非参数估计——已知样本类别，但未知总体概率密度函数形式，要求我们直接推断概率密度函数本身

### 最大似然估计
最大似然估计是进行参数估计的常用方法，它有两个假设：
- 待估计的参数$\theta$是一个确定的、未知的量
- 概率分布函数的形式已知
- 样本为独立抽取

若记$\theta$为待估计参数(比如正态分布中的$\mu,\sigma$),$\mathcal{X}$为样本集合：
$$ \mathcal{X} = \{ x_1,x_2, \dots, x_n\} $$
概率密度函数形式已知，记做$p(x|\theta)$，由于假设样本为独立抽取，则似然函数可以写做：
$$ l(\theta) = p(\mathcal{X}|\theta) = \prod_{i=1}^n p(x_i|\theta)$$
对数似然函数为：
$$  H(\theta) = ln(l(\theta)) = \sum_{i=1}^n lnp(x_i|\theta)$$
首先来看似然函数，它表征了在参数为$\theta$情况下，得到样本集合$\mathcal{X}$的概率，直观来看，这个概率越大，则证明估计得到的参数更好,至于为何使用对数似然函数，这是因为我们需要求$l(\theta)$的最大值，但连乘积求导非常复杂，而取对数运算是单调的，并不改变最值位置，同时计算上大大简化。因此最终最大似然估计值为:

$$  \frac{\partial H(\theta)}{\theta} = 0  $$

实践中常见的概率分布为正态分布，其最大似然估计值为：
$$
\begin{aligned}
    \hat{\mu} &= \frac{1}{n} \sum_{i=1}^n x_i \\\\
    \hat{\sigma^2} &= \frac{1}{n} \sum_{i=1}^n (x_i - \hat{\mu})^2
\end{aligned}
$$
### 贝叶斯估计
首先回顾下贝叶斯决策：
- 状态空间:$\Omega = \{ \omega_1, \omega_2, \dots , \omega_c \}$
- 待识别对象: $\mathbf{x} = [x_1,x_2,\dots, x_d]^T$

设真实状态为$\omega_j$，而采取决策$a_i$所带来的损失为$\lambda(\omega_j,a_i)$,对于特定$\mathbf{x}$，采取决策$a_i$所带来的平均损失为:
$$  R(a_i|\mathbf{x}) = \sum_{j=1}^c \lambda(a_i,\omega_j) p(\omega_j|x)$$
整体损失则需要再对$\mathbf{x}$进行积分，在进行贝叶斯决策时，若想使得整体损失最小，则只需要对任意的$\mathbf{x}$，$R(a_i|\mathbf{x})$。
下面我们考虑参数估计问题:现在有一个样本集$\mathcal{X}$，要求我们找到最优估计量$\hat{\theta}$(对比决策$a_i$),使得带来的贝叶斯风险最小。由此可见，与贝叶斯决策相比，贝叶斯估计也是立足于使得贝叶斯风险最小，两者对比如下图所示：

| 决策问题      | 估计问题  | 
| ------    | -----:  | 
| 样本$\mathbf{x}$       | 样本集$\mathcal{X}$   |   
| 决策量$a_i$        |   待估计值$\hat{\theta}$ |   
| 真实状态$\omega_j$        |    真实参数$\theta$    |
| 状态空间$\mathcal{A}$是离散空间  | 参数空间$\Theta$是连续空间  |
| 先验概率$P(\omega_j)$ | 参数的先验分布$p(\theta)$ |

对于样本集$\mathcal{X}$，参数估计值为$\hat{\theta}$所带来的平均损失为：
$$
    R(\hat{\theta}|\mathcal{X}) = \int_{\Theta} \lambda(\hat{\theta},\theta) p(\hat{\theta}|\mathcal{X}) d\theta
$$
从上式中可以也可以看出贝叶斯决策与贝叶斯估计的区别:
> 贝叶斯决策是立足于每一个样本$\mathbf{x}$，只需要保证对每一个样本均做出最优决策，便可保证整体决策最优；而贝叶斯估计则是立足于样本集$\mathcal{X}$

现在我们要想获得贝叶斯估计，便转化为寻找合适的估计值$\hat{\theta}$,使得$R(\hat{\theta}｜\mathcal{X})$最小，这需要解决两个问题：
- $p(\hat{\theta}|\mathcal{X})$计算
- 损失函数$\lambda(\hat{\theta},\theta)$的选择 

首先考虑$p(\hat{\theta}|\mathcal{X})$的计算，假设$\theta$的先验概率分布为$p(\theta)$,类条件概率密度函数为$p(\mathbf{x}|\theta)$,样本集$\mathcal{X} = \{ x_1,\dots, x_n \}$,似然函数$p(\mathcal{X}|\theta)$可以表示为：
$$
    p(\mathcal{X}|\theta) = \prod_{i=1}^n p(\mathbf{x_i}|\theta)
$$
由贝叶斯公式，后验概率可以表示为:
$$
    p(\hat{\theta}|\mathcal{X}) = \frac{p(\mathcal{X}|\hat{\theta})p(\hat{\theta})}{ \int p(\mathcal{X}|\theta)p(\theta) d\theta}
$$
损失函数$\lambda(\hat{\theta},\theta)$常见的有三种选择:
- 均方误差函数:$\lambda(\hat{\theta},\theta) = (\theta - \hat{\theta})^2$
- 绝对值误差函数: $\lambda(\hat{\theta},\theta) = |\theta - \hat{\theta}|$
- 均匀函数: $\lambda(\hat{\theta}, \theta) = \begin{cases}
    0 & x \in [-\frac{\Delta}{2}, \frac{\Delta}{2}] \\
    1 & other
\end{cases}$

在不同损失函数下，可以推导出不同的结果：
- 均方误差函数:  
$$ \hat{\theta}_{MS} = \int_{-\infty}^\infty \theta p(\theta| \mathcal{X})d\theta$$
- 绝对值误差函数:
$$  \int_{-\infty}^{\hat{\theta_{ABS}}} p(\theta|\mathcal{X}) d\theta = \int_{\hat{\theta_{ABS}}}^{\infty} p(\theta|\mathcal{X}) d\theta $$
- 均匀误差函数: $\theta_{MAP}$应当取在后验概率密度函数最大之处

下面以均方误差函数为例，假设:
- $p(x|\mu) \sim N(\mu, \sigma^2)$
- $p(\mu) \sim N(\mu_0,\sigma_0^2)$ 

则后验概率密度函数$p(\theta|\mathcal{X})$为:
$$
\begin{aligned}
    p(\mu|\mathcal{X}) &= \frac{p(\mathcal{X}|\mu)p(\mu)}{\int p(\mathcal{X}|\mu)p(\mu)d\mu}  \\\\
    &= a exp\{ -\frac{1}{2}[\sum_{k=1}^N (\frac{\mu - x_k}{\sigma})^2 + (\frac{\mu - \mu_0}{\sigma_0})^2] \}
\end{aligned}
$$
由于该概率密度函数是$\mu$的二次函数指数形式，所以仍是一个正态密度，其均值和方差为：
$$
\begin{aligned}
    \mu_N &= \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma^2} \frac{1}{N}\sum_{k=1}^N x_k + \frac{\sigma^2}{N\sigma_0^2 + \sigma^2} \mu_0  \\\\
    \sigma_N^2 &= \frac{\sigma_0^2 \sigma^2}{N\sigma_0^2 + \sigma^2}
\end{aligned}
$$
因为误差函数为均方误差函数，则其参数估计值应当是后验概率密度意义上的均值，即$\mu_N$
### 非参数估计
在使用最大似然估计或者贝叶斯估计进行参数估计时，需要对类条件概率密度函数$p(\mathbf{x}|\omega)$形式已知，但在某些情况下，我们仅仅拥有样本，但并没有这些先验知识，此时若想要进行概率密度函数的估计，则就用到了非参数估计的方法，该类方法具有以下几点假设：
- 不知道分布函数形式
- $N$个样本服从独立同分布

假设样本点$\mathbf{x}$落在$d$维空间中，在$d$维空间中有一个超立方体$\mathcal{R}$,点$\mathbf{x}$落到该超立方体中的概率记做$P$:
$$
    P = \int_{\mathcal{R}} p(x) dx 
$$
若$N$个样本是从密度为$p(x)$的总体中独立抽取的，则$N$个样本中有$k$个落入区域$\mathcal{R}$中的概率符合二项分布，该概率可以写做：
$$
    P_k = C_N^k P^k(1-P)^{N-k}
$$
根据二项分布的性质，$k$的期望为：
$$
    E(k) = NP 
$$
我们的目的是想获得关于$P$的估计值，因此考虑：
$$ \hat{P} = \frac{k}{N}$$
若超立方体$\mathcal{R}$很小，则可以假设在该超球内概率密度函数$p(x)$为定值，此时：
$$
    P = \int_{\mathcal{R}} p(x) dx  = p(x)V 
$$
由此我们便可以获得$p(x)$的估计值：
$$
    \hat{p}(x) = \frac{P}{V} = \frac{k}{NV}
$$
下面讨论下体积$V$的大小及样本数量多少之间的关系。如果把体积$V$固定，样本数取得越来越多，则比值$\frac{k}{N}$将在概率上收敛，最终得到的$p(x)$估计是在超立方体$\mathcal{R}$上的平均估计，而若想要得到$p(x)$的点估计，则需要区间尽可能小，但区间取得过小又会导致一些区间内没有样本点，这种估计也是没有意义的。
下面给出概率密度函数估计值$p_N(x)$收敛于真实概率密度函数$p(x)$的条件：
- $\lim_{N \rightarrow \infty} V_N = 0$
- $\lim_{N \rightarrow \infty} k_N = \infty$
- $\lim_{N \rightarrow \infty} \frac{k_N}{N} = 0$

第一个条件可以使空间平均$\frac{P}{V}$收敛于$p(x)$,第二个条件可以使频数比在概率意义上收敛于概率$P$。满足以上三个条件的区域序列的选择有两种：
- Parzen窗法： 使区域序列$V_N$ 以$N$的某个函数的关系不断缩小，但此时需要对$k_N$和$\frac{k_N}{N}$加些限制条件
- $k_N$近邻估计，让$k_N$为$N$的某个函数，而$V_N$的选取是使相应的$\mathcal{R_N}$正好包含$x$的$k_N$个近邻。

#### Parzen窗法
假设$\mathcal{R_N}$为一个超立方体，其棱长为$h_N$,则此时超立方体体积$V_N = h_N^d$,定义窗函数$\phi(\mu)$：
$$
    \phi(\mu) = \begin{cases}
        1, if |\mu_j| \leq \frac{1}{2} \\
        0, other
    \end{cases}
$$
引入窗函数的的目的主要是对落入区间的样本数$k$进行显示表达，当$x_i$落入以$x$为中心，体积为$V_N$的超立方体内时，$\phi(\mu) = \phi(\frac{x-x_i}{h_N}) = 1$，否则为0，此时落入该超立方体内的样本数为：
$$
    k_N = \sum_{i=1}^N \phi(\frac{x-x_i}{h_N})
$$
由此便可以得到概率密度函数估计：
$$
    p_N(x) = \frac{k_N}{V_NN} = \frac{1}{N} \sum_{i=1}^N \frac{1}{V_N} \phi(\frac{x-x_i}{h_N})
$$
事实上，窗函数的主要作用是内插，反映了每一样本$x_i$对$x$点处的概率密度函数的贡献
下面我们讨论下，若要使得$p_N(x)$满足概率密度函数特性(在整个空间上积分为1),则窗函数应当具有什么特性:
$$
    \begin{aligned}
          \int p_N(x)dx &= \frac{1}{NV_N} \int \sum_{i=1}^N\phi(\frac{x-x_i}{h_N})dx  \\\\
          &= \frac{1}{N} \sum_{i=1}^N \int \phi(\mu)d\mu \\\\
          & = 1
    \end{aligned}
$$
因此考虑到$p_N(x)$非负且积分为1,则窗函数应当具备一般概率密度函数所具有的特征：
- $\phi(\mu) \geq 0 $
- $\int \phi(\mu) d\mu = 1$

常见的窗函数有以下三种：
- 方窗函数
- 正态窗函数:$\phi(\mu) = \frac{1}{\sqrt{2\pi}} exp(-\frac{1}{2}\mu^2)$
- 指数窗函数:$\phi(\mu) = \frac{1}{2} exp(-|\mu|)$

窗宽$h_d$的选择非常重要，若$h_d$选择过大，则会使得估计的分辨力降低，若选的过小，又会导致估计的稳定性下降，反映在估计得到的概率密度函数上就是毛刺很多，一般需要折中考虑。
下面讨论估计量$p_N(x)$的统计性质，可以证明在某些限制条件下，估计量是渐进无偏和平方误差一致的，限制条件为:
- 总体密度函数$p(x)$在$x$点连续
- 窗函数满足以下条件：
    $$ \phi(\mu) \geq 0, \int \phi(\mu) d\mu = 1, sup_{u} \phi(\mu)<\infty$$
- 窗宽满足: $$ \lim_{N \rightarrow \infty} V_N= 0,\lim_{N \rightarrow \infty} NV_N = \infty $$

#### $k_N$近邻估计
parzen窗估计是对体积序列进行限制，而$k_N$近邻估计则是对$k_N$序列做出限制

### 概率密度函数估计的准确性与分类器性能的关系
因为概率密度函数的估计最终还是需要服务于一些下游任务，比如应用贝叶斯决策进行分类，因此这部分主要讨论分类器的误差来源，误差来源主要有以下几种：
- 贝叶斯误差：这种误差是由于不同的类条件概率分布函数之间的相互重叠引起的，这种误差是问题本身所固有的，在分类器设计阶段无法消除
- 模型误差：指因为选择了不正确的模型导致的误差
- 估计误差： 由于采用有限样本进行估计所带来的误差

