---
title: 深度学习基础
date: 2020-09-02 15:24:51
tags: BP算法， 深度学习， CNN 
categories: 统计学习
mathjax: true
toc: true
---
近些年来，神经网络算法家族蓬勃发展，本部分主要介绍一下这些算法的一些通用原理基础，该部分按照以下结构组织:
- 神经元模型
- 多层神经网络
- **反向传播算法(BP)**
- 网络训练中常见问题
- 常见网络模型介绍

<!--more-->

### 神经元模型
时至今日，神经网络已经是一个相当大的、多学科交叉的学科领域，下面给出神经网络的定义：
> **神经网络:** 神经网络是由具有适应性的简单单元所组成的广泛并进行互联的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的交互反应。

神经网络的基本组成成分是神经元模型，即神经网络定义中的“简单单元”。在生物神经网络中，每个神经元与其他神经元相连，当它“兴奋”时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位；如果某神经元的电位超过了一个“阈值”，那么它就会被激活，向其他神经元发送化学物质。

![神经元](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/Neuro-model1.png)
将生物学概念中的神经元模型可以抽象成如下图所示的数学模型，这就是一直沿用至今的“M-P神经元模型”，在这个模型中，神经元接受到来自$n$个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值做比较，然后同过“激活函数”处理以产生神经元的输出。
![神经元模型](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/Neuro-model.jpg)
理想的激活函数是阶跃函数，它将输入值映射为输出值0或1，1对应着神经元兴奋，0对应着神经元抑制，然而阶跃函数具有不连续、不光滑等性质，因此实际常用sigmoid函数作为激活函数:
$$
    sigmoid(x) = \frac{1}{1 + e^{-x}}
$$
sigmoid函数能够将在较大范围内变化的输入值压缩到$(0,1)$输出范围内，因此有时也被称为“挤压函数”。
![激活函数](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/act_function.png)
把许多这样的神经元按照一定的层次结构连接起来，就得到了一个神经网络，尽管神经网络类型有很多，但其组成基本单元几乎都是“M-P神经元模型”。从计算机科学的角度看，我们可以先不考虑神经网络是否真的模拟了神经网络，只需要将一个神经网络视为包含了许多参数的数学模型，这个模型是若干函数，例如$y_j = f(\sum_i w_i x_i - \theta_j)$相互嵌套代入而得，有效的神经网络算法大多以数学证明为支撑。

### 多层感知机
#### 定义
在前面已经介绍过感知机模型，感知机模型也可以看作一个以符号函数为激活函数的神经网络，只有两层，一个是输入层，一个是输出层,如下图所示
![Perceptron](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/Perceptron.jpg)
但是这只是一个线性模型，只能够处理线性可分数据，其学习能力非常有限，甚至对于简单的异或问题都无法解决，更一般的，常见的神经网络是多层结构，如下图所示
![多层感知机](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/MLP.png)
在多层感知机中，每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接，这样的神经网络结构通常称为“多层前馈神经网络”，其中输入层神经元接收外界输出，隐层与输出层神经元对信号进行加工，最终结果由输出层神经网络输出；换言之，输入层神经元仅是接受输入，不进行函数处理，隐层与输出层包含功能神经元，一个神经网络只要包含隐层，就可以称为多层网络。神经网络的学习过程，就是根据训练数据来调整神经元之间的连接权以及每个功能神经元的阈值；换言之，神经网络学到的东西，蕴含在连接权与阈值之中。

#### 多层感知机逼近能力
多层感知机的逼近能力由如下定理保证:
> **定理：** 令$\phi(\cdot)$ 为有界、非常量单调递增连续函数，$I_p$表示$p$维单位超立方体$[0,1]^p$,$C(I_p)$表示定义在$I_p$上的连续函数构成的集合，则给定任意函数$f \in C(I_p)$和$\epsilon >0$ ,存在整数$M$和一组实常数$\alpha_i,\theta_i,w_{ij}$,其中$i=1,2,\dots,M;j=1,2,\dots,p$,使得网络输出:
> $$
    F(x_1,x_2,\dots,x_p) = \sum_{i=1}^M \alpha_i \phi(\sum_{j=1}^p w_{ij}x_j - \theta_i)
> $$
> 可任意逼近$f(\cdot)$,即：
> $$
    |F(x_1,x_2,\dots,x_p) - f(x_1,x_2,\dots,x_p)| < \epsilon , \forall (x_1,\dots,x_p) \in I 
> $$
### 反向传播算法
多层网络的学习能力比单层感知机强得多，欲训练多层网络，则需要更加强大的学习算法，误差逆传播(BP)算法就是其中最杰出的代表，它是迄今为止最成功的神经网络学习算法，现实任务中在使用神经网络时，大多是使用BP算法进行训练。

下面对BP算法做简单推导，给定训练集$D = \{ (\boldsymbol{x_1,y_1}),(\boldsymbol{x_2,y_2}),\dots , (\boldsymbol{x_N,y_N})\}$ ,其中$\boldsymbol{x_i} \in \mathbb{R}^d,\boldsymbol{y_i} \in \mathbb{R}^l$,即输入是由$d$个属性描述，输出$l$维实值向量，下面以一个三层前馈网络来进行反向传播算法推导，输入层有$d$个神经元，隐藏层有$q$个神经元，输出层有$l$个神经元。输出层第$j$个神经元的阈值用$\theta_j$表示，隐层第$h$个神经元的阈值用$\gamma_h$表示，输入层第$i$个神经元与隐层第$h$个神经元之间的连接权重为$v_{ih}$,隐层第$h$个神经元与输出层第$j$个神经元之间的连接权重为$w_{hj}$,记隐层第$h$个神经元接收到的输入为$\alpha_h = \sum_{i=1}^d v_{ih} x_i$,输出层第$j$个神经元输入为$\beta_j = \sum_{h=1}^q w_{hj} b_h $,其中$b_h$为隐层第$h$个神经元的输出。假设隐层和输出层神经元都是用Sigmoid函数作为激活函数。
对训练例$(\boldsymbol{x_k,y_k})$,假定网络输出为$\hat{\boldsymbol{y}}_k = (\hat{y}_1^k, \hat{y}_2^k, \dots, \hat{y}_l^k)$,即：
$$
    \hat{y}_j^k = f(\beta_j - \theta_j)
$$
则网络在$(\boldsymbol{x_k,y_k})$用方差和来定义:
$$
    E_k = \frac{1}{2} \sum_{j=1}^l (\hat{y_j}^k - y_j^k)^2
$$
首先来考虑对于这样一个三层网络，有多少个参数需要确定，权重参数有$d \times q + q \times l$个,阈值参数有$ q + l$个，因此总共需要学习的参数个数为$(d+l+1)q +l$个，BP是一个迭代学习算法，在每一轮中采用广义的感知机策略(链式法则求梯度)进行参数学习，任意参数$v$的更新估计式为:
$$
    v \leftarrow v + \Delta v
$$
具体参数的求解其实就是一个链式法则求导的过程，比如以隐藏层到输出层的权重$w_{hj}$为例，其实只需要找到从$w_{hj}$到误差函数中间有哪些中间变量，然后反向传播时便将误差按照变量路径传递回来即可，对于$w_{hj}$,我们可以写出一条变量路径:
$$
    w_{hj} \rightarrow  \beta_j \rightarrow \hat{y}_j^k \rightarrow E_k
$$ 
因此由链式法则，可得:
$$
    \frac{\partial E_k}{\partial w_{hj}} = \frac{\partial E_k}{\partial \hat{y}_j^k} \frac{ \partial \hat{y}_j^k}{\partial \beta_j} \frac{\partial \beta_j}{\partial w_{hj}} = (\hat{y}_j^k - y_j^k) \hat{y}_j^k (1 - \hat{y}_j^k) b_h
$$
$\frac{ \partial \hat{y}_j^k}{\partial \beta_j} $应用了Sigmoid函数$f'(x) = f(x)(1 - f(x))$的性质，由此梯度下降算法原理，可得：
$$
    \Delta w_{hj} = -\eta \frac{\partial E_k}{\partial w_{hj}} 
$$
下面给出$v_{ih}$的前向变量传递路径：
$$
    v_{ih} \rightarrow \alpha_h \rightarrow b_h \rightarrow \beta_1,\dots,\beta_l \rightarrow \hat{y}_1^k, \hat{y}_2^k \dots \hat{y}_l^k \rightarrow E_k
$$
此时前向路径共有$l$条，因此计算梯度时需要分别极计算$l$部分，计算结果为:
$$
    \begin{aligned}
         \Delta v_{ih} &= -\eta \frac{\partial E_k}{b_h} \frac{\partial b_h}{\partial \alpha_h} \frac{\partial \alpha_h}{\partial v_{ih}}  \\
         &= -\eta [\sum_{j=1}^l \frac{\partial E_k}{\partial \beta_j} \frac{\partial \beta_j}{\partial b_h}] f'(\alpha_h - \gamma_h) x_i 
    \end{aligned}
$$
其他参数更新也都是同样思路，在这里便不做过多赘述，其实BP算法就只是应用链式求导法则反向计算梯度(更新权重)的方法。与随机梯度下降以及批梯度下降相对应，BP算法也可以分为标准BP算法(一次更新一个实例)，累积BP算法(一次更新所有样例)，在实际中训练时往往采取折中的策略，每次取一批数据进行权重更新。

### 网络训练中常见问题
对于这样一个简单BP网络，在训练中也会面临很多问题，这些问题是神经网络算法所固有的一些问题，很多研究也是以解决这些问题为出发点，这部分主要介绍三个问题:
- 过拟合问题
- 梯度消失与梯度爆炸
- 局部极小值问题

#### 过拟合问题
正是由于BP网络具有强大的表示能力，BP神经网络经常遭遇过拟合，即随着训练的进行，训练误差进一步降低，但测试误差却可能上升。有两种策略可以缓解过拟合问题：
- 早停：将数据划分为训练集和验证集，训练集用来计算梯度、更新连接权和阈值，验证集用来估计误差，若训练集误差降低但验证集误差升高，则停止训练，并返回具有最小验证集误差的连接权和阈值。
- 正则化： 在目标误差函数中增加一个用于描述网络复杂度的部分，例如连接权和阈值的平方和，仍令$E_k$表示第$k$个训练样例上的误差，$w_i$表示连接权和阈值，则对于批反向传播的误差函数可以写为:
$$
    E = \lambda \frac{1}{m} \sum_{k=1}^m E_k + (1 - \lambda) \sum_i w_i^2
$$
其中$\lambda$用于对经验误差和网络复杂度进行折中，常通过交叉验证法进行估计。

#### 梯度消失与梯度爆炸
我们在反向计算梯度时实际上是一个链式求导的过程，假设有一个层数为5的神经网络(包含输入层和输出层)，假设我们欲更新输入层到第一隐层的一个权重$w$，中间需要经过若干变量($w_1,\dots,w_k$)才能到达损失函数,则进行反向传播时，损失函数$L$对$w$求偏导是一个连乘积的形式：
$$
    \frac{\partial L}{\partial w} = \frac{\partial L}{\partial w_k} \cdot \frac{\partial w_k}{\partial w_{k-1}} \dots \frac{\partial w_1}{\partial w}
$$
若这些连乘积每一项都大于1，则就出现了梯度爆炸情况，导致参数更新不稳定，若连乘积每一项都小于1，则连乘之后就会是一个非常小的值，也就是说出现了梯度消失。
梯度在反向传播过程中，没经过一层就会经过一次对激活函数求导的过程，加入选的是Sigmoid函数作为激活函数，其导数$f'(x) \leq 0.25$,因此随着网络的变深，使用Sigmoid函数作为激活函数会不可避免地导致出现梯度消失。
![sigmoid](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/sigmoid.png)
下面介绍三个解决梯度消失/爆炸问题的思路。
##### Relu激活函数
为解决使用Sigmoid激活函数导致梯度消失的问题，在深层神经网络的训练中人们往往采用另一种激活函数——Relu以及它的变形Leakage-Relu，函数图像如下图所示:
![relu](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/relu.jpg)
可以看到，Relu函数在被激活后($>0$),梯度恒为1，既不会导致梯度消失也不会导致梯度爆炸，但relu函数有一个明显的缺点:当relu的输入值为负时，输出始终为0，这样会导致神经元不能够更新参数，也就是该神经元无法学习了，为了解决relu函数这个问题，在relu的负半区引入了一个泄漏值，也就是说当输入小于0时仍旧有一个小的梯度，该泄漏值可以指定也可以作为一个参数与神经网络其他参数一同被学习。

##### Batchnorm
机器学习领域有一个非常重要的假设:IID，即独立同分布假设， 假设训练数据和测试数据是满足相同分布的，这也是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障，而：
> **Batchnorm就是在深度神经网络训练过程中使每一层神经网络的输入保持相同分布。**

由于深度神经网络往往具有复杂的网络结构，在进行训练过程中，各层神经元的参数都在变化，导致各层神经元的输入分布不断发生偏移，BN的思想相当直观，之所以训练收敛慢，一般是整体分布逐渐往非线性函数取值区间的上下限两端端靠近，这就导致向后传播时低层神经网络出现梯度消失现象，这是训练神经网络收敛越来越慢的本质原因，而BN就是通过一定规范化手段，把每层神经网络任意神经元的输入值强行拉回到均值为0，方差为1的正态分布，这样使得激活函数的输入落在非线性函数对输入较为敏感的区域(以Sigmoid函数为例，落在0附近区域)，这样进行梯度反向传播时求导得到的梯度值就会较大，一定程度上缓和了梯度消失现象。

但是很明显，从Sigmoid函数图像可以看出，0附近区域其实是接近线性的，如果把所有的输入都拉到Sigmoid函数的线性区域内，那么不就相当于是激活函数是线性函数了么，而我们知道多层线性网络与一层线性网络其实是等价的，所以BN为了保持激活函数的非线性，对norm之后的输入又做了一步变换:
$$
    y_i = \gamma \hat{x}_i +    \beta
$$
每个神经元增加了两个参数：尺度参数$\gamma$，偏移参数$\beta$,这等价于让输入从线性区域向非线性区域移动了一点，引入这两个参数本质上是想找到一个线性和非线性的平衡点，使得网络既具有非线性的强表达能力，又在一定程度上减轻了梯度消失问题。
下面给出Batchnorm的具体流程:
![Batchnorm](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/BN.png)

##### 残差结构
首先来谈一谈深层网络的退化问题，理论上来说，深层网络的性能应当不会比浅层网络性能更差，因为深层网络可以考虑对浅层网络进行拷贝，然后剩下来的层作为一个恒等映射，这样最起码深层网络的性能应当与浅层网络性能一致，然而事实却并非这样，实验表明，深层网络表现并不总是优于浅层网络。
![网络性能对比](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/deep.png)

深度网络的退化说明深度网络不易训练，前面讨论到深层网络往往并不能够通过建立恒等映射的形式来达到与浅层网络相同的性能，因此何凯明等考虑显式的引入该恒等映射，而让中间的网络学习一个残差，假设我们希望一个堆叠非线性层(stacked nonlinear layers)学习的映射是$\mathcal{H(x)}$,我们现在让该堆叠非线性层学习另一个映射$\mathcal{F(x)} = \mathcal{H(x)} - x$，通过$\mathcal{F(x)}+x$来重构期望映射， 而在网络中实现这个目的的操作则是“跳连”(shortcut connection)，如下图所示：
![resnet](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/res.png)

上图所示的结构一般被称为一个残差单元，下面我们来仔细讨论下这样的结构为什么可以解决梯度消失问题，假设上述残差单元的输入为$x$,则残差单元的输出可以表示为:
$$
    \boldsymbol{y} = \mathcal{F(\boldsymbol{x},\{(\boldsymbol{W_i})\})} + \boldsymbol{x}，i=1,2,\dots,k
$$
$k$表示该残差单元包含几个权重层，需要说明的是，一个残差单元至少应该包含两个层，若只有一个层，则会出现:
$$
    \boldsymbol{y} = \boldsymbol{W_1^T x + x} = \boldsymbol{(W_1^T + 1)x}
$$
而若包含两层，则残差单元的输出写做:
$$
    \boldsymbol{y} = \boldsymbol{W_2^T \sigma(W_1^Tx) + \boldsymbol{x}} 
$$
其中,假设$\boldsymbol{x} \in \mathbb{R}^d$,残差单元第一层有$n_1$个神经元，则上式中$\boldsymbol{W_1} \in \mathbb{R}^{d \times n_1}, \boldsymbol{W_2} \in \mathbb{R}^{n_1 \times d}$,损失函数到$\boldsymbol{y}$的梯度为$\frac{\partial L}{\partial \boldsymbol{y}}$,那么我们可以用该梯度表示$\frac{\partial L}{\partial \boldsymbol{x}}$:
$$
    \frac{\partial L}{\partial \boldsymbol{x}} = \frac{\partial L}{\partial \boldsymbol{y}} \frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} = \frac{\partial L}{\partial \boldsymbol{y}} (\boldsymbol{I} +  \boldsymbol{(W_1 W_2)^T}) 
$$
其中激活函数取得Relu激活函数，可以看到，在进行反向传播过程中，到$\boldsymbol{y}$的梯度会无损再传递到其前层的$\boldsymbol{x}$，哪怕是残差梯度等于0或者是一个大于$-1$的负值，也可以保证有梯度传递到前面的层，这使得Resnet相对于普通的神经网络更不容易发生梯度消失现象。
何凯明博士这篇提出Resnet的论文也因为其突出贡献获得了2016年CVPR最佳论文奖，整篇论文非常简洁易懂，非常建议大家去读一下，这里给出论文链接[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

#### 局部极小值问题
神经网络训练过程其实就是一个参数寻优的过程，反向传播算法可以看作是在神经网络这种架构下梯度下降法的实现形式，因此最终收敛的点往往具有梯度为0的特点，这是一个局部极小点，若优化问题是一个凸问题，则局部极小点同时也是全局极小值，而神经网络的优化大多数情况都是一个非凸优化问题，这也就意味着最终训练收敛的点大概率只是一个“局部极小值点”，而非“全局极小值”点,如下图所示:
![局部极小与全局极小](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/min.png)
在现实任务中，人们常采用以下策略试图跳出局部极小，到达全局极小：
- 以多组不同参数值初始化多个网络，按标准方法训练后，取其中误差最小的解作为最终参数，这相当于从多个不同的初始点开始搜索，这样就可能陷入不同的局部极小，从中进行选择有可能获得更接近全局最小的结果。
- 使用“模拟退火”策略，模拟退火在每一步都以一定概率接受比当前解更差的结果，从而有助于跳出局部极小，在迭代过程中，接受“次优解“的概率要随着时间推移而逐渐降低，从而保证算法稳定。
- 使用随机梯度下降(SGD)算法，即便陷入了局部极小点，所计算出的梯度仍可能不为0，这样就有机会跳出局部极小继续搜索。

### 常见网络模型介绍
神经网络家族成员庞杂，除了前面介绍的BP神经网络外，还有很多类型的神经网络，这部分就一些常见的做简要介绍，后面用到的话再补充。
#### 卷积神经网络 
卷积神经网络在图像处理领域有着广泛应用，一个完整的卷积神经网络其实包含两个部分:
- **特征提取**
- 分类

后面分类所用的网络其实就是前面所讨论的全连接神经网络或者它的一些变形，卷积神经网络名字的由来则主要是在特征提取阶段，下面就以图片处理为例重点讲解一下特征提取过程。
![CNN](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/CNN.png)
其实我们不论做什么样的机器学习任务，都可以分为特征提取和算法学习两个阶段，而CNN则将这两个阶段结合了起来，然后一起优化，是一种“end2end”的算法，对于一张$n \times n$像素的黑白图像，其实该图像所有信息都包含在$n \times n$个值当中，若要完成一个分类任务，最简单粗暴的方法可能是直接将这$n \times n$个值直接作为特征输入到到全连接层来进行分类，但是这样做有以下两个缺点:
- 输入特征维度非常高，因此网络的规模会很大，参数量会非常大，计算开销会非常大，训练困难。
- 单个像素值本身并不具备什么意义，最终神经网络结果很有可能并不理想。

卷积神经网络的出发点也是为了解决这样一个问题，特征提取部分主要包含三种操作:
- 卷积层
- ReLu层
- 池化层

#### 卷积层
一个图像中的单个像素点并不具备什么含义，但是一簇像素点(比如一幅图像中的眼睛区域)可能就具备某种含义了，而卷积层的出发点就是希望能够提取图像中某部分区域的信息，而具体操作则是通过卷积核与图像区域进行加权求和，这其实本质上就是一个线性映射，如下图所示，图像处理中的卷积与信号处理中的卷积不同之处在于其仅仅只有加权求和操作，而并没有翻转操作。
![图像卷积](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/conv.png)
上图中所示的是一个$3 \times 3$的卷积核，可以看出该卷积核对于一个区域左上角像素与有下降像素更加关注，而一正一负操作则表明该卷积核更关注的是左上角像素值与由下角像素值的差。选用不同的卷积核可以从原始图像中提取不同的特征，因此在实际应用中，一般会选用多个大小不一的卷积核以全面捕捉不同大小的区域所包含的信息。
#### ReLu层
经过Relu层则主要是为了实现非线性变换，以提高网络的表示能力，比如上图中的-8经过ReLu层之后就变成了0。
#### 池化层
经过卷积操作之后，我们得到了一张张有着不同值的特征图(feature map)，尽管数据量比原图少了很多，但还是过于庞大，而池化层的作用就是进一步减少数据量,扩大特征图的感受野。
![池化](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/Pooling.png)
池化一般有两种操作，一种是Max pooling,一种是Average pooling,前一种就是取一个区域的最大值，而后一种则是取一个区域的平均值。
#### 总结
以上这三种操作就是就是卷积神经网络特征提取部分的三个基本单元，对这三种基本单元进行组合便可以组成特征提取部分，需要说明的是卷积神经网络中一个非常重要的操作就是**共享权重**：
> 假设输入图像为$32 \times 32$,用两个$5 \times 5$