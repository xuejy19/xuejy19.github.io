---
title: 深度学习基础
date: 2020-09-02 15:24:51
tags: BP算法， 深度学习， CNN 
categories: 统计学习
mathjax: true
toc: true
---
近些年来，神经网络算法家族蓬勃发展，本部分主要介绍一下这些算法的一些通用原理基础，该部分按照以下结构组织:
- 神经元模型
- 多层神经网络
- **反向传播算法(BP)**
- 目前常用网络模型介绍

<!--more-->

### 神经元模型
时至今日，神经网络已经是一个相当大的、多学科交叉的学科领域，下面给出神经网络的定义：
> **神经网络:** 神经网络是由具有适应性的简单单元所组成的广泛并进行互联的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的交互反应。

神经网络的基本组成成分是神经元模型，即神经网络定义中的“简单单元”。在生物神经网络中，每个神经元与其他神经元相连，当它“兴奋”时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位；如果某神经元的电位超过了一个“阈值”，那么它就会被激活，向其他神经元发送化学物质。

![神经元](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/Neuro-model1.png)
将生物学概念中的神经元模型可以抽象成如下图所示的数学模型，这就是一直沿用至今的“M-P神经元模型”，在这个模型中，神经元接受到来自$n$个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值做比较，然后同过“激活函数”处理以产生神经元的输出。
![神经元模型](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/Neuro-model.jpg)
理想的激活函数是阶跃函数，它将输入值映射为输出值0或1，1对应着神经元兴奋，0对应着神经元抑制，然而阶跃函数具有不连续、不光滑等性质，因此实际常用sigmoid函数作为激活函数:
$$
    sigmoid(x) = \frac{1}{1 + e^{-x}}
$$
sigmoid函数能够将在较大范围内变化的输入值压缩到$(0,1)$输出范围内，因此有时也被称为“挤压函数”。
![激活函数](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/act_function.png)
把许多这样的神经元按照一定的层次结构连接起来，就得到了一个神经网络，尽管神经网络类型有很多，但其组成基本单元几乎都是“M-P神经元模型”。从计算机科学的角度看，我们可以先不考虑神经网络是否真的模拟了神经网络，只需要将一个神经网络视为包含了许多参数的数学模型，这个模型是若干函数，例如$y_j = f(\sum_i w_i x_i - \theta_j)$相互嵌套代入而得，有效的神经网络算法大多以数学证明为支撑。

### 多层感知机
#### 定义
在前面已经介绍过感知机模型，感知机模型也可以看作一个以符号函数为激活函数的神经网络，只有两层，一个是输入层，一个是输出层,如下图所示
![Perceptron](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/Perceptron.jpg)
但是这只是一个线性模型，只能够处理线性可分数据，其学习能力非常有限，甚至对于简单的异或问题都无法解决，更一般的，常见的神经网络是多层结构，如下图所示
![多层感知机](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/MLP.png)
在多层感知机中，每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接，这样的神经网络结构通常称为“多层前馈神经网络”，其中输入层神经元接收外界输出，隐层与输出层神经元对信号进行加工，最终结果由输出层神经网络输出；换言之，输入层神经元仅是接受输入，不进行函数处理，隐层与输出层包含功能神经元，一个神经网络只要包含隐层，就可以称为多层网络。神经网络的学习过程，就是根据训练数据来调整神经元之间的连接权以及每个功能神经元的阈值；换言之，神经网络学到的东西，蕴含在连接权与阈值之中。

#### 多层感知机逼近能力
多层感知机的逼近能力由如下定理保证:
> **定理：** 令$\phi(\cdot)$ 为有界、非常量单调递增连续函数，$I_p$表示$p$维单位超立方体$[0,1]^p$,$C(I_p)$表示定义在$I_p$上的连续函数构成的集合，则给定任意函数$f \in C(I_p)$和$\epsilon >0$ ,存在整数$M$和一组实常数$\alpha_i,\theta_i,w_{ij}$,其中$i=1,2,\dots,M;j=1,2,\dots,p$,使得网络输出:
> $$
    F(x_1,x_2,\dots,x_p) = \sum_{i=1}^M \alpha_i \phi(\sum_{j=1}^p w_{ij}x_j - \theta_i)
> $$
> 可任意逼近$f(\cdot)$,即：
> $$
    |F(x_1,x_2,\dots,x_p) - f(x_1,x_2,\dots,x_p)| < \epsilon , \forall (x_1,\dots,x_p) \in I 
> $$
### 反向传播算法
多层网络的学习能力比单层感知机强得多，欲训练多层网络，则需要更加强大的学习算法，误差逆传播(BP)算法就是其中最杰出的代表，它是迄今为止最成功的神经网络学习算法，现实任务中在使用神经网络时，大多是使用BP算法进行训练。

下面对BP算法做简单推导，给定训练集$D = \{ (\boldsymbol{x_1,y_1}),(\boldsymbol{x_2,y_2}),\dots , (\boldsymbol{x_N,y_N})\}$ ,其中$\boldsymbol{x_i} \in \mathbb{R}^d,\boldsymbol{y_i} \in \mathbb{R}^l$,即输入是由$d$个属性描述，输出$l$维实值向量，下面以一个三层前馈网络来进行反向传播算法推导，输入层有$d$个神经元，隐藏层有$q$个神经元，输出层有$l$个神经元。输出层第$j$个神经元的阈值用$\theta_j$表示，隐层第$h$个神经元的阈值用$\gamma_h$表示，输入层第$i$个神经元与隐层第$h$个神经元之间的连接权重为$v_{ih}$,隐层第$h$个神经元与输出层第$j$个神经元之间的连接权重为$w_{hj}$,记隐层第$h$个神经元接收到的输入为$\alpha_h = \sum_{i=1}^d v_{ih} x_i$,输出层第$j$个神经元输入为$\beta_j = \sum_{h=1}^q w_{hj} b_h $,其中$b_h$为隐层第$h$个神经元的输出。假设隐层和输出层神经元都是用Sigmoid函数作为激活函数。
对训练例$(\boldsymbol{x_k,y_k})$,假定网络输出为$\hat{\boldsymbol{y}}_k = (\hat{y}_1^k, \hat{y}_2^k, \dots, \hat{y}_l^k)$,即：
$$
    \hat{y}_j^k = f(\beta_j - \theta_j)
$$
则网络在$(\boldsymbol{x_k,y_k})$用方差和来定义:
$$
    E_k = \frac{1}{2} \sum_{j=1}^l (\hat{y_j}^k - y_j^k)^2
$$
首先来考虑对于这样一个三层网络，有多少个参数需要确定，权重参数有$d \times q + q \times l$个,阈值参数有$ q + l$个，因此总共需要学习的参数个数为$(d+l+1)q +l$个，BP是一个迭代学习算法，在每一轮中采用广义的感知机策略(链式法则求梯度)进行参数学习，任意参数$v$的更新估计式为:
$$
    v \leftarrow v + \Delta v
$$
具体参数的求解其实就是一个链式法则求导的过程，比如以隐藏层到输出层的权重$w_{hj}$为例，其实只需要找到从$w_{hj}$到误差函数中间有哪些中间变量，然后反向传播时便将误差按照变量路径传递回来即可，对于$w_{hj}$,我们可以写出一条变量路径:
$$
    w_{hj} \rightarrow  \beta_j \rightarrow \hat{y}_j^k \rightarrow E_k
$$ 
因此由链式法则，可得:
$$
    \frac{\partial E_k}{\partial w_{hj}} = \frac{\partial E_k}{\partial \hat{y}_j^k} \frac{ \partial \hat{y}_j^k}{\partial \beta_j} \frac{\partial \beta_j}{\partial w_{hj}} = (\hat{y}_j^k - y_j^k) \hat{y}_j^k (1 - \hat{y}_j^k) b_h
$$
其中$\frac{ \partial \hat{y}_j^k}{\partial \beta_j} $应用了Sigmoid函数$f'(x) = f(x)(1 - f(x))$的性质，由此梯度下降算法原理，可得：
$$
    \Delta w_{hj} = -\eta \frac{\partial E_k}{\partial w_{hj}} 
$$
下面给出$v_{ih}$的前向变量传递路径：
$$
    v_{ih} \rightarrow \alpha_h \rightarrow b_h \rightarrow \beta_1,\dots,\beta_l \rightarrow \hat{y}_1^k, \hat{y}_2^k \dots \hat{y}_l^k \rightarrow E_k
$$
此时前向路径共有$l$条，因此计算梯度时需要分别极计算$l$部分，计算结果为:
$$
    \begin{aligned}
         \Delta v_{ih} &= -\eta \frac{\partial E_k}{b_h} \frac{\partial b_h}{\partial \alpha_h} \frac{\partial \alpha_h}{\partial v_{ih}}  \\
         &= -\eta [\sum_{j=1}^l \frac{\partial E_k}{\partial \beta_j} \frac{\partial \beta_j}{\partial b_h}] f'(\alpha_h - \gamma_h) x_i 
    \end{aligned}
$$
其他参数更新也都是同样思路，在这里便不做过多赘述，其实BP算法就只是应用链式求导法则反向计算梯度(更新权重)的方法。与随机梯度下降以及批梯度下降相对应，BP算法也可以分为标准BP算法(一次更新一个实例)，累积BP算法(一次更新所有样例)，在实际中训练时往往采取折中的策略，每次取一批数据进行权重更新。
正是由于BP网络具有强大的表示能力，BP神经网络经常遭遇过拟合，即随着训练的进行，训练误差进一步降低，但测试误差却可能上升。过拟合