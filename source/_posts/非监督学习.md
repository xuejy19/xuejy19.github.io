---
title: 非监督学习
date: 2020-09-11 15:48:31
tags: PCA, MDS, ISOMAP 
categories: 统计学习 
mathjax: true
toc: true 
---
这部分总结下传统统计学习的最后一部分内容——非监督学习，非监督学习又称无监督学习，我们前面讲的大部分算法都是需要样本的标签的，通过标签来构造损失函数，进而进行模型学习，但在有的情况下我们并没有数据的标签，这种情况下我们就什么都不能做了么？当然不是，对于无标签数据，我们仍旧可以开展一部分工作，比如：
- 概率模型估计 
- 数据降维
- 数据聚类

由于概率模型估计算法在前面已经讲过了([EM算法](https://xuejy19.github.io/2020/07/30/EM/)、[概率密度函数估计](https://xuejy19.github.io/2020/07/27/概率密度函数估计/#more))，这里就不再赘述，本文按照以下结构组织:
- 无监督学习思想
- 数据降维
- 数据聚类 

<!--more-->

### 无监督学习思想
无监督学习是从无标注的数据中学习数据的统计规律或者说内在结构的机器学习，主要包括聚类、降维、概率估计，**无监督学习可以用于数据分析或者监督学习预处理**。
无监督学习使用无标注数据$U = \{ x_1, x_2, \dots, x_N\}$学习或训练，其中$x_i，i = 1,2,\dots,N$, 是样本，由特征向量组成。无监督学习的模型是函数$z = g_{\theta}(x)$ , 条件概率分布$P_{\theta}(z|x)$，或条件概率分布$P_{\theta}(x|z)$。其中$x \in X$是输入，表示样本； $z \in Z$是输出，表示对样本的分析结果，可以是类别、转换、概率；$\theta$是参数。 
无监督学习是一个困难的任务，因为数据没有标注，也就是没有人为的引导，机器需要自己从数据中找出规律。模型的输入$x$在数据中可以观测，而输出$z$隐藏在数据中。无监督学习通常需要大量的数据，因为对数据隐藏的规律的发现需要足够的观测。 
![机器学习算法划分](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/Unsupervised.jpg)

### 数据降维
#### 问题定义 
降维是将训练数据中的样本(实例)从高维空间转换到低维空间。假设样本原本存在于低维空间，或者近似地存在于低维空间，通过降维则可以更好地表示样本数据的结构，即更好地表示样本之间的关系。高维空间通常是高维的欧式空间，而低维空间则是低维的欧式空间或者流形(manifold)。低维空间不是事先给定，而是从数据中自动发现，其维数通常是事先给定的。从高维到低维的降维中，要保证样本的信息损失最少。
![降维](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/dimension.png)

本文主要介绍四种降维方法：
- 主成分分析法(PCA)
- 多维缩放(MDS)
- 等距特征映射(ISOMAP) 
- 局部线性
### 数据聚类 
#### 问题定义 
聚类是将样本集合中相似的样本(实例)分配到相同的类，不相似的样本分配到不同的类。聚类时，样本通常是欧式空间中的向量，类别不是事先给定，而是从数据中自动发现，但**类别的个数**通常要预先给定。样本之间的相似度或距离由**度量**决定。如果一个样本只能属于一个类，则称为硬聚类；如果一个样本可以属于多个类，则称为软聚类。
![聚类](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/clustering.png)