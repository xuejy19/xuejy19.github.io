---
title: 非监督学习
date: 2020-09-11 15:48:31
tags: PCA, MDS, ISOMAP 
categories: 统计学习 
mathjax: true
#img: https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/thumbnail/unsupervise.png
toc: true 
---
这部分总结下传统统计学习的最后一部分内容——非监督学习，非监督学习又称无监督学习，我们前面讲的大部分算法都是需要样本的标签的，通过标签来构造损失函数，进而进行模型学习，但在有的情况下我们并没有数据的标签，这种情况下我们就什么都不能做了么？当然不是，对于无标签数据，我们仍旧可以开展一部分工作，比如：
- 概率模型估计 
- 数据降维
- 数据聚类

由于概率模型估计算法在前面已经讲过了([EM算法](https://xuejy19.github.io/2020/07/30/EM/)、[概率密度函数估计](https://xuejy19.github.io/2020/07/27/概率密度函数估计/#more))，这里就不再赘述，本文按照以下结构组织:
- 无监督学习思想
- 数据降维
- 数据聚类 

<!--more-->

### 无监督学习思想
无监督学习是从无标注的数据中学习数据的统计规律或者说内在结构的机器学习，主要包括聚类、降维、概率估计，**无监督学习可以用于数据分析或者监督学习预处理**。
无监督学习使用无标注数据$U = \{ x_1, x_2, \dots, x_N\}$学习或训练，其中$x_i，i = 1,2,\dots,N$, 是样本，由特征向量组成。无监督学习的模型是函数$z = g_{\theta}(x)$ , 条件概率分布$P_{\theta}(z|x)$，或条件概率分布$P_{\theta}(x|z)$。其中$x \in X$是输入，表示样本； $z \in Z$是输出，表示对样本的分析结果，可以是类别、转换、概率；$\theta$是参数。 
无监督学习是一个困难的任务，因为数据没有标注，也就是没有人为的引导，机器需要自己从数据中找出规律。模型的输入$x$在数据中可以观测，而输出$z$隐藏在数据中。无监督学习通常需要大量的数据，因为对数据隐藏的规律的发现需要足够的观测。 
![机器学习算法划分](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/Unsupervised.jpg)

### 数据降维
#### 问题定义 
降维是将训练数据中的样本(实例)从高维空间转换到低维空间。假设样本原本存在于低维空间，或者近似地存在于低维空间，通过降维则可以更好地表示样本数据的结构，即更好地表示样本之间的关系。高维空间通常是高维的欧式空间，而低维空间则是低维的欧式空间或者流形(manifold)。低维空间不是事先给定，而是从数据中自动发现，其维数通常是事先给定的。从高维到低维的降维中，要保证样本的信息损失最少。
![降维](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/dimension.png)

本文主要介绍四种降维方法：
- 主成分分析法(PCA)
- 多维缩放(MDS)
- 等距特征映射(ISOMAP) 
- 局部线性嵌入

在详述各种降维方法之前，我们需要明确这样一件事：
> 进行降维的过程实际上也是进行特征提取的过程，选择什么样的降维算法取决于我们期望提出哪些特征，比如我们希望提取的特征能够尽可能将样本点区分开来，就可以采用PCA


#### 主成分分析法(PCA)
首先我们考虑这样一个问题：
> 对于正交属性空间中的样本点，如何用一个超平面对所有样本进行恰当的表达？

我们直观上来思考这个问题，若存在这样的超平面，那么它大概具有这样的性质:
- 最近重构性：样本点到这个超平面的距离都足够近
- 最大可分性： 样本点在这个超平面上的投影能够尽可能分开 

有趣的是，从上面两个角度出发，能分别得到主成分分析的两种等价推导，PCA算法的推导有一个基本前提:
> **前提假设：假设样本数据进行了中心化，即$\sum_i x_i = 0$** 

现在考虑样本空间$R^d$中的一组基$\{ w_1, w_2, \dots, w_d \}$, 其中$w_i$是标准正交基向量，$||w_i||_2 = 1, w_i^Tw_j = 0(i \neq j)$,若丢弃新坐标系中的部分坐标，即将维度降低到$d' < d$,则样本点$x_i$在低维坐标系中的投影是$z_i = (z_{i1}, z_{i2}, \dots, z_{id'})$,其中$z_{ij} = w_j^T x_i$是$x_i$在低维坐标系下第$j$维的坐标，若基于$z_i$来重构$x_i$,则会得到$\hat{x}_i = \sum_{j=1}^{d'} z_{ij} w_j$,基于上面叙述，我们下面从两个角度来推导主成分分析。
##### 最近重构性 
假设有$m$个样本点，则从原样本点到到基于投影重构的样本点$\hat{x}_i$的距离平方的均值为:
$$
    \begin{aligned}
        \frac{1}{m}\sum_{i=1}^m || x_i - \hat{x}_i||^2   &=  \frac{1}{m}\sum_{i=1}^m|| \sum_{j=1}^{d'} z_{ij}w_j - x_i||^2   \\
        &= \frac{1}{m} (\sum_{i=1}^m \sum_{j=1}^{d'} z_{ij}^2 - 2\sum_{i=1}^m \sum_{j=1}^{d'} z_{ij} w_j^T x_i  + \sum_{i=1}^m x_i^T x_i)  \\
        &= \frac{1}{m}(\sum_{i=1}^m \sum_{j=1}^{d'} w_j^T x_i x_i^T w_j - 2 \sum_{i=1}^m \sum_{j=1}^{d'} w_j^T x_i x_i^T w_j + \sum_{i=1}^m x_i^Tx_i) \\
        &= -\frac{1}{m}(\sum_{i=1}^m \sum_{j=1}^{d'} w_j^T x_i x_i^T w_j + const) \\
        &= -tr(W^T \frac{1}{m} \sum_{i=1}^m x_i x_i^T W) 
    \end{aligned}
$$
其中$W = (w_1, w_2, \dots, w_{d'})$, 因为我们数据已经做了零均值化，因此$\frac{1}{m}\sum_{i=1}^m x_i x_i^T$刚好是样本集协方差矩阵，不妨记$\Sigma = \frac{1}{m} \sum_{i=1}^m x_i x_i^T$,因此从这个角度导出来的优化问题为:
$$
    \begin{aligned}
        &\max_{W} tr(W^T \Sigma W) \\
        &s.t. W^T W = I_{d' \times d'}
    \end{aligned}
$$

### 数据聚类 
#### 问题定义 
聚类是将样本集合中相似的样本(实例)分配到相同的类，不相似的样本分配到不同的类。聚类时，样本通常是欧式空间中的向量，类别不是事先给定，而是从数据中自动发现，但**类别的个数**通常要预先给定。样本之间的相似度或距离由**度量**决定。如果一个样本只能属于一个类，则称为硬聚类；如果一个样本可以属于多个类，则称为软聚类。
![聚类](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/clustering.png)
