---
title: 聚类算法
date: 2020-09-21 16:02:08
tags: kmeans，层次聚类，谱聚类 
categories: 统计学习 
toc: true 
mathjax: true
---
聚类是将样本集合中相似的样本(实例)分配到相同的类，不相似的样本分配到不同的类。聚类时，样本通常是欧式空间中的向量，类别不是事先给定，而是从数据中自动发现，但**类别的个数**通常要预先给定。样本之间的相似度或距离由**度量**决定。如果一个样本只能属于一个类，则称为硬聚类；如果一个样本可以属于多个类，则称为软聚类。
<!--more-->
![聚类](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/clustering.png)

在这一部分主要介绍四种聚类算法:
- $k$均值聚类 
- 层次聚类
- 谱聚类 
- DBSCAN 

### $k$均值聚类 
#### 算法思想 
$k$均值聚类的思想非常直观:
> 如果一团样本分布的足够紧凑，那么就可以认为这些样本属于一类，而一团样本的紧凑程度可以通过这些样本距离该团样本中心的距离和来衡量，距离和越小则说明该团样本越紧凑。

#### 算法推导 
假设对于样本集$\mathcal{X}$, 初始时随机将样本分成$c$类，那么根据$k$均值的算法思想可以定义损失函数为:
$$
    J(e) = \sum_{i=1}^c \sum_{y \in \Gamma_i} ||y - m_i||^2
$$
公式中$m_i$是目前划归第$i$类样本的均值。想要求得使$J(e)$最小的最优样本划分并不容易，是一个NP难的问题，因此在优化时采用贪心策略，通过迭代优化的形式来进行求解。在迭代过程中，每次移动一个样本点来使$J(e)$减小，为此我们需要计算将一个样本点$y$从$\Gamma_i$类转移到$\Gamma_k$类对损失函数的影响，因为损失函数是依类别进行计算然后累加的，不妨将$\Gamma_i$的类的损失记做$J_i$。在将$y$移动之后，$\Gamma_i$和$\Gamma_k$类均发生了变化，均值分别变为:
$$
    \begin{aligned}
        m'_i &= m_i + \frac{m_i - y}{N_i - 1} \\
        m'_k &= m_k  + \frac{y - m_k}{N_k + 1}
    \end{aligned}
$$
据此，可以分别计算出$J'_i$和$J'_k$并用$J_i, J_k$表示:
$$
    \begin{aligned}
        J'_i &= J_i - \frac{N_i}{N_i - 1} ||y - m_i||^2 \\ 
        J'_k &= J_k + \frac{N_k}{N_k + 1} ||y - m_k||^2 
    \end{aligned}
$$
如果$J'_i > J'_k$，就将样本从$i$类移动到$k$类，由此便可以得到$k$均值聚类步骤:
> **$k$均值:** 
> - Step1: 把样本初始划分成$C$类，并计算$J_e$ 
> - Step2: 选择一个样本$y$，假设$y \in \Gamma_i$ 
> - Step3: 若$N_i = 1$, 则转Step2 
> - Step4: 计算
> $$
     \rho_j = \begin{cases}
         \frac{N_j}{N_j + 1} ||y - m_j||^2 & j \neq i \\
         \frac{N_i}{N_i - 1} ||y - m_i||^2 & j = i
     \end{cases}
> $$
> - Step5: 若对$\forall j, \rho_k \leq \rho_j$, 则将$y$从$\Gamma_i$移动到$\Gamma_k$中去。
> - Step6: 修正$m_i,m_k$和$J_e$ 
> - Step2: 若连续迭代$N$次(将样本遍历一遍)不变，则算法终止，否则回到Step2

#### 算法分析 
##### 算法时间复杂度
假设算法迭代$t$次后收敛，样本个数为$n$，样本维数为$d$， 类别数目为$k$，则算法时间复杂度为:
$$
    O(tndk)
$$
##### 算法超参数 
算法要求预先制定聚类类别个数$k$

##### 局部极小 
因为$k$均值算法本质上是贪心算法，因此很有可能陷入局部最优，可以通过更新聚类个数以及不同的初始化来一定程度上减轻局部最优，来获得较好的聚类效果。 


### 层次聚类

#### 算法思想 
层次聚类首先将各个样本点划分为1类，然后将距离最近的两类进行合并，建立一个新的类，重复这个步骤直到聚类结果满足要求。 
从定义来看，层次聚类算法是自下而上进行聚类，在应用该聚类算法时需要明确三个问题:
- 相似性度量的选择
- 合并规则，即如何定义两个类别的相似程度 
- 停止条件  

下面就这三个关键问题进行分析。

#### 算法详述 

##### 相似性度量
任何一个聚类算法会面临相似性度量的选择问题，实际中常用的相似性度量有:
- 欧式距离 
- 马氏距离 
- 测地距离
- ...... 

具体选择什么样的度量来刻画相似度需要根据实际数据样本的分布情况和待解决的问题来确定。 

##### 合并规则 
合并规则一般是类间距离最小，若记$\Delta(\Gamma_i, \Gamma_j)$为类间距离，$\delta(x,y)$,为$x$和$y$之间的距离，则类间距离一般有以下几种定义:
- 最近距离 
$$
    \Delta(\Gamma_i, \Gamma_j) = \min_{y \in \Gamma_i, \tilde{y} \in \Gamma_j} \delta(y, \tilde{y})
$$
- 最远距离 
$$
    \Delta(\Gamma_i, \Gamma_j) = \max_{y \in \Gamma_i, \tilde{y} \in \Gamma_j} \delta(y, \tilde{y})
$$
- 均值距离
$$
    \Delta(\Gamma_i, \Gamma_j) = \delta(m_i, m_j)
$$

##### 停止条件 
如果不加停止条件，则最终所有样本都会归于一类，停止条件一般有2种选择:
- 类别个数: 当类别个数到达期望值$k$时，便停止合并，聚类结束。 
- 类间距离: 当类间距离大于某个值时便停止合并，聚类结束。

#### 算法流程
![聚类树](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/hc.png)
层次聚类算法流程总结如下: 
> **层次聚类:**
> **输入:** $n$个样本组成的样本集合及样本之间的距离 
> **输出:** 对样本集合的一个层次化聚类 
> **算法流程:**
> - 根据选定的相似性度量方法，计算$n$个样本两两之间的距离$d_{ij}$,得到距离矩阵$D$ 
> - 构造$n$个类，每个类只包含一个样本 
> - 合并类间距离最小的两个类，构成一个新类，并计算新类与当前各类之间的距离。
> - 若满足聚类终止条件，则聚类终止，否则返回第3步 

### 谱聚类 
#### 算法思想
谱聚类算法是从图论中衍生出的一种算法，后来在聚类问题中得到了广泛的应用。它的主要思想是把所有的数据看做空间中的点，这些点之间可以用边连接起来。距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高，通过对所有数据点组成的图进行切图，让切图后不同的子图间边权重和尽可能的低，而子图内的边权重和尽可能的高，从而达到聚类的目的。

学习谱聚类算法主要需要理清两个问题:
- 无向权重图的相关定义 
- 图的切割任务 

下面就按这个思路来对谱聚类算法进行介绍。 

#### 无向权重图 

对于我们的样本集$\mathcal{X} \subseteq \mathbb{R}^d$，可以将每一个样本点看作高维空间中的一个点，若将这些点以某种方式连接起来则构成了一个图，如果对于相连的两个节点$x$和$y$，若并不区分$x \rightarrow y$和$y \rightarrow x$，则构成了一个无向图。
![无向图](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/AG.jpg)

但对于我们的聚类任务，我们期望距离较近的点它们之间的连接权重大一些，距离较远的样本点之间的权重小一些么，从这样的一个想法出发，我们有以下几种方式来构造相似性图:
- $k$近邻图: 若$v_i$是$v_j$的$k$近邻，则$v_i$和$v_j$之间存在一条边，边上权重设置为1。
- 对称$k$近邻图: 若两个点互为$k$近邻，则这两个点之间存在一条边。
- $\epsilon$-近邻图: 任意两个距离小于$\epsilon$的点之间存在一条边 
- 全连接图: 相似性大于0的两个点之间均存在一条边，常用高斯核函数来进行相似性度量:
$$
    W_{ij} = S_{ij} = \exp(- \frac{||x_i - x_j||^2}{2 \sigma^2}) 
$$

下面给出一些符号定义:
- $w_{ij}$： 边权重 
- 节点$v_i \in V$的度: $d_i = \sum_{j=1}^n w_{ij}$ 
- 度矩阵: $D = diag(d_1,d_2, \dots, d_n)$ 
- 加权邻接矩阵: $W = (w_{ij})_{i,j= 1,\dots,n}$  


#### 图的切割任务 
在已经定义好权重无向图的情况下，现在我们考虑这样一个任务: 
> 将已经得到的图进行划分，使得不同点集边的权重较小，而同一点集内的边权重较大。以这种思想来完成聚类任务。 

直接想到的损失函数是如下形式:
$$
    \begin{aligned}
        cut(A_1,A_2,\dots, A_k) &:= \frac{1}{2} \sum_{i=1}^k W(A_i, \bar{A}_i) \\
        W(A, B) &:= \sum_{i \in A, j \in B} w_{ij} 
    \end{aligned}
$$
但选择这样形式的损失函数，最后往往是有一类包含非常多的点， 其他类都是单独的点。以聚成两类为例，假设有12个点，若两类各6个点，则最终计算损失函数时，需要将$2*6*6=72$项求和，若一类11个点，一类1个点，则最终计算时只需要将$2*1*11=22$项求和，这也是为何选用上述损失函数容易得到单独的点。 
我们在聚类时，往往期望各个类别的样本点个数比较均衡，因此考虑在损失函数中对样本点较少的类别进行惩罚，由此便可得到两种损失函数定义:
$$
    \begin{aligned}
        RatioCut(A_1, A_2, \dots, A_k) &:= \frac{1}{2} \sum_{i=1}^k \frac{W(A_i, \bar{A}_i)}{|A_i|} = \sum_{i=1}^k \frac{cut(A_i, \bar{A}_i)}{|A_i|} \\
        Ncut(A_1,A_2, \dots, A_k) &:= \frac{1}{2} \sum_{i=1}^k \frac{W(A_i, \bar{A}_i)}{Vol(A_i)} = \sum_{i=1}^k \frac{cut(A_i, \bar{A}_i)}{Vol(A_i)}
    \end{aligned}
$$