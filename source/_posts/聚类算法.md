---
title: 聚类算法
date: 2020-09-21 16:02:08
tags: kmeans，层次聚类，谱聚类 
categories: 统计学习 
toc: true 
mathjax: true
---
聚类是将样本集合中相似的样本(实例)分配到相同的类，不相似的样本分配到不同的类。聚类时，样本通常是欧式空间中的向量，类别不是事先给定，而是从数据中自动发现，但**类别的个数**通常要预先给定。样本之间的相似度或距离由**度量**决定。如果一个样本只能属于一个类，则称为硬聚类；如果一个样本可以属于多个类，则称为软聚类。
<!--more-->
![聚类](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/clustering.png)

在这一部分主要介绍四种聚类算法:
- $k$均值聚类 
- 层次聚类
- 谱聚类 
- DBSCAN 

### $k$均值聚类 
#### 算法思想 
$k$均值聚类的思想非常直观:
> 如果一团样本分布的足够紧凑，那么就可以认为这些样本属于一类，而一团样本的紧凑程度可以通过这些样本距离该团样本中心的距离和来衡量，距离和越小则说明该团样本越紧凑。

#### 算法推导 
假设对于样本集$\mathcal{X}$, 初始时随机将样本分成$c$类，那么根据$k$均值的算法思想可以定义损失函数为:
$$
    J(e) = \sum_{i=1}^c \sum_{y \in \Gamma_i} ||y - m_i||^2
$$
公式中$m_i$是目前划归第$i$类样本的均值。想要求得使$J(e)$最小的最优样本划分并不容易，是一个NP难的问题，因此在优化时采用贪心策略，通过迭代优化的形式来进行求解。在迭代过程中，每次移动一个样本点来使$J(e)$减小，为此我们需要计算将一个样本点$y$从$\Gamma_i$类转移到$\Gamma_k$类对损失函数的影响，因为损失函数是依类别进行计算然后累加的，不妨将$\Gamma_i$的类的损失记做$J_i$。在将$y$移动之后，$\Gamma_i$和$\Gamma_k$类均发生了变化，均值分别变为:
$$
    \begin{aligned}
        m'_i &= m_i + \frac{m_i - y}{N_i - 1} \\
        m'_k &= m_k  + \frac{y - m_k}{N_k + 1}
    \end{aligned}
$$
据此，可以分别计算出$J'_i$和$J'_k$并用$J_i, J_k$表示:
$$
    \begin{aligned}
        J'_i &= J_i - \frac{N_i}{N_i - 1} ||y - m_i||^2 \\ 
        J'_k &= J_k + \frac{N_k}{N_k + 1} ||y - m_k||^2 
    \end{aligned}
$$
如果$J'_i > J'_k$，就将样本从$i$类移动到$k$类，由此便可以得到$k$均值聚类步骤:
> **$k$均值:** 
> - Step1: 把样本初始划分成$C$类，并计算$J_e$ 
> - Step2: 选择一个样本$y$，假设$y \in \Gamma_i$ 
> - Step3: 若$N_i = 1$, 则转Step2 
> - Step4: 计算
> $$
     \rho_j = \begin{cases}
         \frac{N_j}{N_j + 1} ||y - m_j||^2 & j \neq i \\
         \frac{N_i}{N_i - 1} ||y - m_i||^2 & j = i
     \end{cases}
> $$
> - Step5: 若对$\forall j, \rho_k \leq \rho_j$, 则将$y$从$\Gamma_i$移动到$\Gamma_k$中去。
> - Step6: 修正$m_i,m_k$和$J_e$ 
> - Step2: 若连续迭代$N$次(将样本遍历一遍)不变，则算法终止，否则回到Step2

#### 算法分析 
##### 算法时间复杂度
假设算法迭代$t$次后收敛，样本个数为$n$，样本维数为$d$， 类别数目为$k$，则算法时间复杂度为:
$$
    O(tndk)
$$
##### 算法超参数 
算法要求预先制定聚类类别个数$k$

##### 局部极小 
因为$k$均值算法本质上是贪心算法，因此很有可能陷入局部最优，可以通过更新聚类个数以及不同的初始化来一定程度上减轻局部最优，来获得较好的聚类效果。 


### 层次聚类

#### 算法思想 
层次聚类首先将各个样本点划分为1类，然后将距离最近的两类进行合并，建立一个新的类，重复这个步骤直到聚类结果满足要求。 
从定义来看，层次聚类算法是自下而上进行聚类，在应用该聚类算法时需要明确三个问题:
- 相似性度量的选择
- 合并规则，即如何定义两个类别的相似程度 
- 停止条件  

下面就这三个关键问题进行分析。

#### 算法详述 

##### 相似性度量
任何一个聚类算法会面临相似性度量的选择问题，实际中常用的相似性度量有:
- 欧式距离 
- 马氏距离 
- 测地距离
- ...... 

具体选择什么样的度量来刻画相似度需要根据实际数据样本的分布情况和待解决的问题来确定。 

##### 合并规则 
合并规则一般是类间距离最小，若记$\Delta(\Gamma_i, \Gamma_j)$为类间距离，$\delta(x,y)$,为$x$和$y$之间的距离，则类间距离一般有以下几种定义:
- 最近距离 
$$
    \Delta(\Gamma_i, \Gamma_j) = \min_{y \in \Gamma_i, \tilde{y} \in \Gamma_j} \delta(y, \tilde{y})
$$
- 最远距离 
$$
    \Delta(\Gamma_i, \Gamma_j) = \max_{y \in \Gamma_i, \tilde{y} \in \Gamma_j} \delta(y, \tilde{y})
$$
- 均值距离
$$
    \Delta(\Gamma_i, \Gamma_j) = \delta(m_i, m_j)
$$

##### 停止条件 
如果不加停止条件，则最终所有样本都会归于一类，停止条件一般有2种选择:
- 类别个数: 当类别个数到达期望值$k$时，便停止合并，聚类结束。 
- 类间距离: 当类间距离大于某个值时便停止合并，聚类结束。

#### 算法流程
![聚类树](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/hc.png)
层次聚类算法流程总结如下: 
> **层次聚类:**
> **输入:** $n$个样本组成的样本集合及样本之间的距离 
> **输出:** 对样本集合的一个层次化聚类 
> **算法流程:**
> - 根据选定的相似性度量方法，计算$n$个样本两两之间的距离$d_{ij}$,得到距离矩阵$D$ 
> - 构造$n$个类，每个类只包含一个样本 
> - 合并类间距离最小的两个类，构成一个新类，并计算新类与当前各类之间的距离。
> - 若满足聚类终止条件，则聚类终止，否则返回第3步 

### 谱聚类 

