---
title: 降维算法
date: 2020-09-21 15:58:24
tags: PCA, ISOMAP, MDS, LLE 
categories: 统计学习 
toc: true 
mathjax: true 
---
降维是将训练数据中的样本(实例)从高维空间转换到低维空间。假设样本原本存在于低维空间，或者近似地存在于低维空间，通过降维则可以更好地表示样本数据的结构，即更好地表示样本之间的关系。高维空间通常是高维的欧式空间，而低维空间则是低维的欧式空间或者流形(manifold)。低维空间不是事先给定，而是从数据中自动发现，其维数通常是事先给定的。从高维到低维的降维中，要保证样本的信息损失最少。
<!--more-->
![降维](https://raw.githubusercontent.com/xuejy19/xuejy19.github.io/source/Img/dimension.png)

本文主要介绍四种降维方法：
- 主成分分析法(PCA)
- 多维缩放(MDS)
- 等距特征映射(ISOMAP) 
- 局部线性嵌入

在详述各种降维方法之前，我们需要明确这样一件事：
> 进行降维的过程实际上也是进行特征提取的过程，选择什么样的降维算法取决于我们期望提出哪些特征，比如我们希望提取的特征能够尽可能将样本点区分开来，就可以采用PCA


### 主成分分析法(PCA)
#### 问题描述
首先我们考虑这样一个问题：
> 对于正交属性空间中的样本点，如何用一个超平面对所有样本进行恰当的表达？

我们直观上来思考这个问题，若存在这样的超平面，那么它大概具有这样的性质:
- 最近重构性：样本点到这个超平面的距离都足够近
- 最大可分性： 样本点在这个超平面上的投影能够尽可能分开 

有趣的是，从上面两个角度出发，能分别得到主成分分析的两种等价推导，PCA算法的推导有一个基本前提:
> **前提假设：假设样本数据进行了中心化，即$\sum_i x_i = 0$** 

现在考虑样本空间$R^d$中的一组基$\{ w_1, w_2, \dots, w_d \}$, 其中$w_i$是标准正交基向量，$||w_i||_2 = 1, w_i^Tw_j = 0(i \neq j)$,若丢弃新坐标系中的部分坐标，即将维度降低到$d' < d$,则样本点$x_i$在低维坐标系中的投影是$z_i = (z_{i1}, z_{i2}, \dots, z_{id'})$,其中$z_{ij} = w_j^T x_i$是$x_i$在低维坐标系下第$j$维的坐标，若基于$z_i$来重构$x_i$,则会得到$\hat{x}_i = \sum_{j=1}^{d'} z_{ij} w_j$,基于上面叙述，我们下面从两个角度来推导主成分分析。
##### 最近重构性 
假设有$m$个样本点，则从原样本点到到基于投影重构的样本点$\hat{x}_i$的距离平方的均值为:
$$
    \begin{aligned}
        \frac{1}{m}\sum_{i=1}^m || x_i - \hat{x}_i||^2   &=  \frac{1}{m}\sum_{i=1}^m|| \sum_{j=1}^{d'} z_{ij}w_j - x_i||^2   \\
        &= \frac{1}{m} (\sum_{i=1}^m \sum_{j=1}^{d'} z_{ij}^2 - 2\sum_{i=1}^m \sum_{j=1}^{d'} z_{ij} w_j^T x_i  + \sum_{i=1}^m x_i^T x_i)  \\
        &= \frac{1}{m}(\sum_{i=1}^m \sum_{j=1}^{d'} w_j^T x_i x_i^T w_j - 2 \sum_{i=1}^m \sum_{j=1}^{d'} w_j^T x_i x_i^T w_j + \sum_{i=1}^m x_i^Tx_i) \\
        &= -\frac{1}{m}(\sum_{i=1}^m \sum_{j=1}^{d'} w_j^T x_i x_i^T w_j + const) \\
        &= -tr(W^T \frac{1}{m} \sum_{i=1}^m x_i x_i^T W) 
    \end{aligned}
$$
其中$W = (w_1, w_2, \dots, w_{d'})$, 因为我们数据已经做了零均值化，因此$\frac{1}{m}\sum_{i=1}^m x_i x_i^T$刚好是样本集协方差矩阵，不妨记$\Sigma = \frac{1}{m} \sum_{i=1}^m x_i x_i^T$,因此从这个角度导出来的优化问题为:
$$
    \begin{aligned}
        &\max_{W} tr(W^T \Sigma W) \\
        &s.t. W^T W = I_{d' \times d'}
    \end{aligned}
$$

##### 最大可分性
若投影向量基选为$W = (w_1, w_2, \dots, w_{d'})$,则样本点在新空间的坐标为$W^T x_i$,根据最大可分性的定义，我们期望所有样本点的投影能够尽可能分开，也就是说投影后的样本集的方差尽可能大，因为对原始空间的样本点做了0均值化，因此投影后样本集的均值也是零，协方差矩阵则是：
$$
    \frac{1}{m} \sum_{i=1}^m W^T x_i x_i^T W 
$$
我们的优化目标是向各个坐标轴投影的坐标方差尽可能大，目标函数可以写做:
$$
    \frac{1}{m} \sum_{i=1}^m \sum_{j=1}^{d'} w_j^T x_i x_i^T w_j = tr(W^T \Sigma W) 
$$
因此优化问题可以写做:
$$
    \begin{aligned}
        & \max_{W} tr(W^T \Sigma W) \\ 
        & s.t. W^TW = I_{d' \times d'}
    \end{aligned} 
$$

从优化问题推导来看，从两个角度进行推导，我们得到了相同的优化问题。 

#### 优化问题求解 
该优化问题是带等式约束的优化问题，考虑采用Lagrange乘子法求解，首先写出Lagrange函数:
$$
    L = \sum_{i=1}^{d'} w_i^T \Sigma w_i - \sum_{i=1}^m \lambda_i (w_i^T w_i - 1)
$$
需要注意的是，该Lagrange乘子法的写法相当于是将原始优化问题放松了，因为少了$w_i$两两彼此正交的条件，因此我们通过该Lagrange乘子法求得最优解后，需验证其是否满足正交性。 让$L$对$w_i$求偏导并令偏导为0:
$$
    \frac{\partial L}{\partial w_i} = 2 \Sigma w_i - 2 \lambda_i w_i = 0 
$$
由此我们得到了$w_i$应当满足的条件:
$$
  \Sigma w_i = \lambda_i w_i 
$$
上式说明$w_i$应当是矩阵$\Sigma$的特征向量，而其对应的特征值为$\lambda_i$,将该结论回代目标优化式可得:
$$
    target = \sum_{i=1}^{d'} \lambda_i 
$$
因此若想让目标函数尽可能大，则应当选取矩阵$\Sigma$的较大的特征值所对应的特征向量作为投影向量。据此我们便可得到PCA算法的流程:
>**主成分分析法：** 
>**输入：** 样本集$D = \{ x_1, x_2, \dots, x_m\}$, 低维空间维数$d'$ 
>**输出：** 投影矩阵$W^{\ast} = [w_1, w_2, \dots, w_{d'}]$ 
>**步骤：** 
>- 对所有样本点进行中心化$x_i \leftarrow x_i - \frac{1}{m} \sum_{i=1}^m x_i$ 
>- 计算样本的协方差矩阵$\Sigma = \frac{1}{m} X X^T$ 
>- 对协方差矩阵$\Sigma$做特征值分解 
>- 取最大的$d'$个特征值所对应的特征向量构成投影矩阵 

对于主成分分析法，我们可以从多个角度进行理解，从重构损失最小的角度来看，由于原始数据维度很高，但可能其主要的信息仅仅只在几个主成分方向上，因此沿主成分方向进行分解，则可以在保留尽可能多信息的情况下对数据进行降维。从最大可分性的角度来看，将原始空间的样本向主成分方向投影，可以使得样本在这个方向上能够尽可能分开,从这个角度我们对比一下线性判别分析(LDA),LDA是有监督的降维方法，希望不同类别的样本点能够在某个方向上尽可能分开，而PCA则是无监督的降维，是针对整个样本集，期望样本集在某个方向上投影方差尽可能大。

#### 降维维数$d'$的确定
降维后低维空间的维数$d'$通常需要用户事先指定， 或者对于特定任务，通过交叉验证的方法来选取较好的维数，但如果我们从重构损失的角度来看，我们往往期望$\hat{x}$能够尽可能接近$x$，而如何衡量重构效果呢？这里就与信息论相一致，认为一个变量方差越大，则其所包含的信息越多，因此就通过累积方差来横量重构效果，比如$\Sigma$最大的特征值所对应的特征向量为$w_1$，则将原始空间样本点向该特征向量投影，所得到的值的方差为:
$$
    w_1^T \Sigma w_1 =  \lambda_1 w_1^T w_1 = \lambda_1
$$
而所谓累积方差则是因为我们将原始空间的点向$d'$个主成分方向投影，在这$d'$个方向上的方差和为:
$$
    \sum_{i=1}^{d'} \lambda_i
$$
贡献率则是在$d'$个方向上投影的累积方差与向所有$d$个方向投影的累积方差之比:
$$
    contribute = \frac{\sum_{i=1}^{d'} \lambda_i}{\sum_{i=1}^d \lambda_i}
$$
因此在选择降维维数时，我们也可以从重构贡献率的角度入手，限定贡献率阈值来选取降维维数$d'$。

#### PCA降维有效性分析 
使用PCA将样本点从$d$维降到$d'$维必然会导致信息的损失，这是因为对应于$\Sigma$最小的$d - d'$个特征值所对应的特征向量被舍去了，但有时舍弃这部分信息是必要的，因为:
- 原始空间维度过大，但信息仅仅只在某些主成分方向，通过降维可以便于数据传输，降低传输难度
- 舍弃这部分信息能够使得样本的采样密度增大 
- 当数据受到噪声影响时，小的特征值所对应的特征向量往往与噪声有关， 舍弃它们能在一定程度上起到去噪的效果。